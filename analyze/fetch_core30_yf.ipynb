{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a8afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] meta saved: /Users/hiroyukiyamanaka/Desktop/python_stock/dash_plotly/data/parquet/core30_meta.parquet rows=30\n",
      "[OK] prices (max_1d) saved: /Users/hiroyukiyamanaka/Desktop/python_stock/dash_plotly/data/parquet/core30_prices_max_1d.parquet rows=181286\n",
      "[OK] prices (max_1wk) saved: /Users/hiroyukiyamanaka/Desktop/python_stock/dash_plotly/data/parquet/core30_prices_max_1wk.parquet rows=37922\n",
      "[OK] prices (max_1mo) saved: /Users/hiroyukiyamanaka/Desktop/python_stock/dash_plotly/data/parquet/core30_prices_max_1mo.parquet rows=8750\n",
      "[OK] prices (730d_1h) saved: /Users/hiroyukiyamanaka/Desktop/python_stock/dash_plotly/data/parquet/core30_prices_730d_1h.parquet rows=152925\n",
      "[OK] prices (60d_5m) saved: /Users/hiroyukiyamanaka/Desktop/python_stock/dash_plotly/data/parquet/core30_prices_60d_5m.parquet rows=117089\n",
      "[OK] prices (60d_15m) saved: /Users/hiroyukiyamanaka/Desktop/python_stock/dash_plotly/data/parquet/core30_prices_60d_15m.parquet rows=41243\n",
      "[OK] manifest updated: /Users/hiroyukiyamanaka/Desktop/python_stock/dash_plotly/data/parquet/manifest.json (entries=7)\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_meta.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_prices_max_1d.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_prices_max_1wk.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_prices_max_1mo.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_prices_730d_1h.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_prices_60d_5m.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_prices_60d_15m.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/manifest.json\n",
      "[OK] S3 upload done (count=8)\n"
     ]
    }
   ],
   "source": [
    "# === Core30 prices pipeline (meta-based) ===\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# --- add for module path (nbconvert runs with cwd=analyze) ---\n",
    "ROOT = Path.cwd().resolve().parent  # project root\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# ==== 共通設定/ユーティリティ（既存モジュールを使用） ====\n",
    "from common_cfg.env import load_dotenv_cascade\n",
    "from common_cfg.flags import NO_MANIFEST, NO_S3\n",
    "from common_cfg.paths import (\n",
    "    PARQUET_DIR,\n",
    "    MASTER_META_PARQUET,\n",
    "    MANIFEST_PATH,\n",
    "    PRICE_SPECS,\n",
    "    price_parquet,\n",
    ")\n",
    "from common_cfg.s3cfg import DATA_BUCKET, PARQUET_PREFIX, AWS_REGION, AWS_PROFILE\n",
    "from common_cfg.manifest import sha256_of, write_manifest_atomic\n",
    "from common_cfg.s3io import maybe_upload_files_s3\n",
    "\n",
    "# .env 読み込み（.env.s3 → .env の順で存在すれば読み込み）\n",
    "load_dotenv_cascade()\n",
    "\n",
    "# ==== Core30 抽出（meta.parquet ベース） ====\n",
    "META_PATH = MASTER_META_PARQUET\n",
    "if not META_PATH.exists():\n",
    "    raise FileNotFoundError(f\"not found: {META_PATH}\")\n",
    "\n",
    "meta_df = pd.read_parquet(META_PATH, engine=\"pyarrow\")\n",
    "required_cols = {\"code\", \"stock_name\", \"ticker\", \"tag1\"}\n",
    "missing = required_cols.difference(meta_df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"meta parquet missing columns: {sorted(missing)}\")\n",
    "\n",
    "core = (\n",
    "    meta_df.loc[meta_df[\"tag1\"].astype(\"string\").str.upper() == \"TOPIX_CORE30\",\n",
    "                [\"code\", \"stock_name\", \"ticker\"]]\n",
    "    .dropna(subset=[\"ticker\"])\n",
    "    .drop_duplicates(subset=[\"ticker\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "core[\"code\"] = core[\"code\"].astype(\"string\")\n",
    "core[\"stock_name\"] = core[\"stock_name\"].astype(\"string\")\n",
    "core[\"ticker\"] = core[\"ticker\"].astype(\"string\")\n",
    "\n",
    "if core.empty:\n",
    "    raise RuntimeError(\"Core30 list is empty. Check tag1 values in meta.parquet.\")\n",
    "\n",
    "tickers: list[str] = core[\"ticker\"].tolist()\n",
    "\n",
    "# === 出力先（複数 period/interval 対応） ===\n",
    "FILES_TO_GENERATE = PRICE_SPECS\n",
    "\n",
    "PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === フラット化（スキーマ維持 + 欠損抑制） ===\n",
    "# === フラット化（スキーマ維持 + 欠損抑制） ===\n",
    "def _flatten_multi(raw: pd.DataFrame, tickers: list[str], interval: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    yfinance.download を用いて単階層のOHLCV(+Adj Close)へ整形。\n",
    "    5m/15m/1h: JST（Asia/Tokyo）に変換してから naive へ落とす。\n",
    "    1d/1wk/1mo: UTC 0:00 のまま（タイムゾーン情報のみ削除）。\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    if isinstance(raw.columns, pd.MultiIndex):\n",
    "        lv0 = raw.columns.get_level_values(0)\n",
    "        for t in tickers:\n",
    "            if t in lv0:\n",
    "                sub = raw[t].copy()\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                sub = sub.reset_index()\n",
    "                if \"Datetime\" in sub.columns:\n",
    "                    sub = sub.rename(columns={\"Datetime\": \"date\"})\n",
    "                elif \"Date\" in sub.columns:\n",
    "                    sub = sub.rename(columns={\"Date\": \"date\"})\n",
    "                elif \"index\" in sub.columns:\n",
    "                    sub = sub.rename(columns={\"index\": \"date\"})\n",
    "                else:\n",
    "                    sub.columns = [\"date\"] + [c for c in sub.columns[1:]]\n",
    "                sub[\"ticker\"] = t\n",
    "                keep = [c for c in [\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"ticker\"] if c in sub.columns]\n",
    "                sub = sub[keep].copy()\n",
    "                frames.append(sub)\n",
    "    else:\n",
    "        sub = raw.reset_index()\n",
    "        if \"Datetime\" in sub.columns:\n",
    "            sub = sub.rename(columns={\"Datetime\": \"date\"})\n",
    "        elif \"Date\" in sub.columns:\n",
    "            sub = sub.rename(columns={\"Date\": \"date\"})\n",
    "        elif \"index\" in sub.columns:\n",
    "            sub = sub.rename(columns={\"index\": \"date\"})\n",
    "        sub[\"ticker\"] = tickers[0] if tickers else \"UNKNOWN\"\n",
    "        keep = [c for c in [\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"ticker\"] if c in sub.columns]\n",
    "        sub = sub[keep].copy()\n",
    "        frames.append(sub)\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"ticker\"])\n",
    "\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # date列の処理\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    \n",
    "    # interval に応じてタイムゾーン処理を分岐\n",
    "    if interval in (\"5m\", \"15m\", \"1h\"):\n",
    "        # 5m/15m/1h: JST に変換\n",
    "        try:\n",
    "            if out[\"date\"].dt.tz is not None:\n",
    "                out[\"date\"] = out[\"date\"].dt.tz_convert(\"Asia/Tokyo\")\n",
    "            else:\n",
    "                out[\"date\"] = out[\"date\"].dt.tz_localize(\"UTC\").dt.tz_convert(\"Asia/Tokyo\")\n",
    "            out[\"date\"] = out[\"date\"].dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            try:\n",
    "                out[\"date\"] = out[\"date\"].dt.tz_localize(None)\n",
    "            except Exception:\n",
    "                pass\n",
    "    else:\n",
    "        # 1d/1wk/1mo: UTC 0:00 のまま（タイムゾーン情報のみ削除）\n",
    "        try:\n",
    "            out[\"date\"] = out[\"date\"].dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 型統一\n",
    "    for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    # 欠損抑制: dateがNaTは除外 / OHLCのいずれかが欠損する行は除外（Volumeは欠損許容）\n",
    "    out = out[out[\"date\"].notna()].copy()\n",
    "    need_ohlc = [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in out.columns]\n",
    "    if need_ohlc:\n",
    "        out = out.dropna(subset=need_ohlc, how=\"any\")\n",
    "\n",
    "    # 重複除去の前に整列\n",
    "    if \"ticker\" in out.columns:\n",
    "        out = out.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        out = out.sort_values([\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _fetch_prices(period: str, interval: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    period × interval 指定で yfinance を取得し、スキーマを揃えて返す。\n",
    "    auto_adjust は明示的に True（欠落や将来のデフォルト変更に影響されない）。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw = yf.download(\n",
    "            tickers,\n",
    "            period=period,\n",
    "            interval=interval,\n",
    "            group_by=\"ticker\",\n",
    "            threads=True,\n",
    "            progress=False,\n",
    "            auto_adjust=True,  # ← 明示\n",
    "        )\n",
    "        df = _flatten_multi(raw, tickers, interval)\n",
    "        if df.empty:\n",
    "            raise RuntimeError(\"yf.download returned empty. fallback to per-ticker.\")\n",
    "    except Exception:\n",
    "        frames = []\n",
    "        for t in tickers:\n",
    "            try:\n",
    "                r = yf.download(\n",
    "                    t,\n",
    "                    period=period,\n",
    "                    interval=interval,\n",
    "                    group_by=\"ticker\",\n",
    "                    threads=True,\n",
    "                    progress=False,\n",
    "                    auto_adjust=True,  # ← 明示\n",
    "                )\n",
    "                f = _flatten_multi(r, [t], interval)\n",
    "                if not f.empty:\n",
    "                    frames.append(f)\n",
    "            except Exception:\n",
    "                pass\n",
    "        df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "    need = {\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"ticker\"}\n",
    "    if df.empty or not need.issubset(df.columns):\n",
    "        raise RuntimeError(f\"No price data collected or required columns missing for period={period} interval={interval}.\")\n",
    "    return df\n",
    "\n",
    "def _append_or_create(path: Path, new_df: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    追記のみ運用:\n",
    "      - 既存があれば読み込み→縦結合→(ticker,date)で重複排除（欠損の少ない行を優先）→上書き保存\n",
    "      - なければ新規保存\n",
    "    戻り値: 書き出し後の総行数\n",
    "    \"\"\"\n",
    "    cols = [\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"ticker\"]\n",
    "    if path.exists():\n",
    "        try:\n",
    "            existing = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "        except Exception:\n",
    "            existing = pd.DataFrame(columns=cols)\n",
    "    else:\n",
    "        existing = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # 列補完（最小限の型安全）\n",
    "    for c in cols:\n",
    "        if c not in new_df.columns:\n",
    "            new_df[c] = pd.Series(dtype=existing[c].dtype if c in existing.columns else \"float64\")\n",
    "        if c not in existing.columns:\n",
    "            existing[c] = pd.Series(dtype=new_df[c].dtype)\n",
    "\n",
    "    # FutureWarning 回避: 空でないものだけを concat\n",
    "    frames = []\n",
    "    if not existing.empty:\n",
    "        frames.append(existing[cols])\n",
    "    if not new_df.empty:\n",
    "        frames.append(new_df[cols])\n",
    "\n",
    "    if frames:\n",
    "        both = pd.concat(frames, ignore_index=True)\n",
    "    else:\n",
    "        both = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # 冪等 & 欠損抑制:\n",
    "    #  - (ticker,date) 重複時は「欠損カウントが少ない行」を優先\n",
    "    #  - その後、OHLC 欠損行は除外（Volume は許容）\n",
    "    if not both.empty:\n",
    "        na_cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in both.columns]\n",
    "        both[\"_na\"] = both[na_cols].isna().sum(axis=1)\n",
    "        both = both.sort_values([\"ticker\",\"date\",\"_na\"], ascending=[True, True, True]).drop_duplicates(\n",
    "            subset=[\"ticker\",\"date\"], keep=\"first\"\n",
    "        )\n",
    "        both = both.drop(columns=[\"_na\"], errors=\"ignore\")\n",
    "        # 最終ガード（OHLC 欠損を弾く）\n",
    "        need_ohlc = [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in both.columns]\n",
    "        if need_ohlc:\n",
    "            both = both.dropna(subset=need_ohlc, how=\"any\")\n",
    "        both = both.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    both.to_parquet(path, engine=\"pyarrow\", index=False)\n",
    "    return len(both)\n",
    "\n",
    "# ==== 保存 ====\n",
    "print(f\"[INFO] core30 universe size: {len(core)}\")\n",
    "written_files: list[Path] = []\n",
    "\n",
    "# 各ファイルを生成\n",
    "for period, interval in FILES_TO_GENERATE:\n",
    "    suffix = f\"{period}_{interval}\"\n",
    "    out_path = price_parquet(period, interval)\n",
    "    df_iv = _fetch_prices(period=period, interval=interval)\n",
    "    n_rows = _append_or_create(out_path, df_iv)\n",
    "    print(f\"[OK] prices ({suffix}) saved: {out_path} rows={n_rows}\")\n",
    "    written_files.append(out_path)\n",
    "\n",
    "# ==== manifest 更新（新リストで上書き；旧 1y_1d のキーは消滅） ====\n",
    "if not NO_MANIFEST:\n",
    "    items = []\n",
    "    for p in written_files:\n",
    "        stat = p.stat()\n",
    "        items.append({\n",
    "            \"key\": p.name,  # ← ファイル名そのまま（S3キーではない）\n",
    "            \"bytes\": stat.st_size,\n",
    "            \"sha256\": sha256_of(p),\n",
    "            \"mtime\": pd.Timestamp(stat.st_mtime, unit=\"s\", tz=\"UTC\").isoformat(),\n",
    "        })\n",
    "    write_manifest_atomic(items, MANIFEST_PATH)\n",
    "    print(f\"[OK] manifest updated: {MANIFEST_PATH} (entries={len(items)})\")\n",
    "else:\n",
    "    print(\"[INFO] PIPELINE_NO_MANIFEST=1 → manifest 更新はスキップ\")\n",
    "\n",
    "# ==== S3 アップロード（抑止可能）====\n",
    "_to_upload = (written_files + [MANIFEST_PATH]) if not NO_MANIFEST else written_files\n",
    "maybe_upload_files_s3(\n",
    "    _to_upload,\n",
    "    bucket=DATA_BUCKET,\n",
    "    prefix=PARQUET_PREFIX,\n",
    "    aws_region=AWS_REGION,\n",
    "    aws_profile=AWS_PROFILE,\n",
    "    dry_run=False\n",
    ")\n",
    "print(f\"[OK] S3 upload done (count={len(_to_upload)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b25f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}