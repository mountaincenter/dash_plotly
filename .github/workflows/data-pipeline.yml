name: Data Pipeline

on:
  schedule:
    # æ¯Žæ—¥23æ™‚ï¼ˆJSTï¼‰ã«å®Ÿè¡Œï¼ˆåœŸæ—¥ç¥å«ã‚€ï¼‰
    # Groké¸å®šã¯ææ–™ã®é®®åº¦ãŒé‡è¦ãªãŸã‚ã€æ¯Žæ—¥å®Ÿè¡Œ
    # GitHub Actions cron ã¯å¸¸ã«UTCæ™‚åˆ»ã§æŒ‡å®š
    # JST = UTC + 9æ™‚é–“ ã®å¤‰æ›ã‚’åŽ³å®ˆ
    - cron: "45 7 * * *" # UTC 07:45 = JST 16:45 (å½“æ—¥) - æ ªä¾¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã®ã¿ï¼ˆTOPIXç³»16:30æ›´æ–°å¾Œï¼‰
    - cron: "0 14 * * *" # UTC 14:00 = JST 23:00 (å½“æ—¥) - Groké¸å®š + æ ªä¾¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°
  workflow_dispatch:
    inputs:
      skip_trading_day_check:
        description: "å–¶æ¥­æ—¥ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«é–¢ä¿‚ãªãå¼·åˆ¶å®Ÿè¡Œï¼‰"
        type: boolean
        default: false
      force_meta_jquants:
        description: "meta_jquants.parquetã‚’å¼·åˆ¶æ›´æ–°ï¼ˆ7æ—¥é–“ã®é®®åº¦ãƒã‚§ãƒƒã‚¯ã‚’ç„¡è¦–ï¼‰"
        type: boolean
        default: false
      force_grok:
        description: "GrokéŠ˜æŸ„é¸å®šã‚’å¼·åˆ¶å®Ÿè¡Œï¼ˆgenerate_grok_trending.pyã‚’å®Ÿè¡Œï¼‰"
        type: boolean
        default: false
      force_1645_mode:
        description: "16:45ãƒ¢ãƒ¼ãƒ‰ã‚’å¼·åˆ¶ï¼ˆGroké¸å®šã‚¹ã‚­ãƒƒãƒ—ã€ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼‰"
        type: boolean
        default: false

permissions:
  id-token: write
  contents: read

jobs:
  pipeline:
    runs-on: ubuntu-latest
    environment: AWS_OIDC

    env:
      AWS_REGION: ${{ vars.AWS_REGION || 'ap-northeast-1' }}
      S3_BUCKET: ${{ vars.DATA_BUCKET || vars.S3_BUCKET }}
      S3_PREFIX: ${{ vars.PARQUET_PREFIX || 'parquet/' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup J-Quants authentication
        run: |
          # Create .env.jquants for J-Quants API v2 authentication
          cat > .env.jquants <<EOF
          JQUANTS_API_KEY=${{ secrets.JQUANTS_API_KEY }}
          EOF

          echo "âœ… J-Quants v2 authentication configured"

      - name: Force execution notice
        if: ${{ github.event.inputs.skip_trading_day_check == 'true' }}
        run: |
          echo "âš ï¸  FORCE EXECUTION MODE"
          echo "Trading day check has been bypassed via manual dispatch"
          echo "Pipeline will run regardless of schedule"

      - name: Determine execution mode
        id: exec_mode
        run: |
          CURRENT_HOUR=$(TZ=Asia/Tokyo date +%H)
          echo "Current hour (JST): $CURRENT_HOUR"

          # force_1645_mode ãŒ true ã®å ´åˆã€å¼·åˆ¶çš„ã«16:45ãƒ¢ãƒ¼ãƒ‰ï¼ˆGroké¸å®šã‚¹ã‚­ãƒƒãƒ—ï¼‰
          if [ "${{ github.event.inputs.force_1645_mode }}" = "true" ]; then
            echo "mode=stock_data_forced" >> $GITHUB_OUTPUT
            echo "skip_trading_check=true" >> $GITHUB_OUTPUT
            echo "skip_grok=true" >> $GITHUB_OUTPUT
            echo "ðŸ’¹ FORCED 16:45 Mode - Grok selection SKIPPED, backtest ENABLED"
          # force_grok ãŒ true ã®å ´åˆã€å¼·åˆ¶çš„ã«GROKãƒ¢ãƒ¼ãƒ‰
          elif [ "${{ github.event.inputs.force_grok }}" = "true" ]; then
            echo "mode=grok_forced" >> $GITHUB_OUTPUT
            echo "skip_trading_check=true" >> $GITHUB_OUTPUT
            echo "skip_grok=false" >> $GITHUB_OUTPUT
            echo "ðŸš€ FORCED GROK Mode - Manual GROK selection execution"
          # 23æ™‚ï¼ˆ14:00 UTCï¼‰ã®å®Ÿè¡Œã¯GROKãƒ¢ãƒ¼ãƒ‰ï¼ˆå–¶æ¥­æ—¥ãƒã‚§ãƒƒã‚¯ãªã—ã€Groké¸å®šå®Ÿè¡Œï¼‰
          elif [ "$CURRENT_HOUR" = "23" ]; then
            echo "mode=grok_daily" >> $GITHUB_OUTPUT
            echo "skip_trading_check=true" >> $GITHUB_OUTPUT
            echo "skip_grok=false" >> $GITHUB_OUTPUT
            echo "ðŸ“Š GROK Daily Mode (23:00 JST) - Grok selection enabled"
          else
            echo "mode=stock_data" >> $GITHUB_OUTPUT
            echo "skip_trading_check=false" >> $GITHUB_OUTPUT
            echo "skip_grok=true" >> $GITHUB_OUTPUT
            echo "ðŸ’¹ Stock Data Mode (16:45 JST) - Grok selection skipped"
          fi

      - name: Check trading day window
        id: check_trading
        if: ${{ github.event.inputs.skip_trading_day_check != 'true' && steps.exec_mode.outputs.skip_trading_check != 'true' }}
        env:
          JQUANTS_API_KEY: ${{ secrets.JQUANTS_API_KEY }}
        run: |
          echo "ðŸ” Checking if within trading day execution window..."
          python scripts/check_trading_day.py
          EXIT_CODE=$?

          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… Within execution window (16:45-02:00 JST on trading day)"
          else
            echo "âŒ Outside execution window or non-trading day"
          fi

          exit $EXIT_CODE

      - name: Check next day is trading day (for 23:00 run)
        id: check_next_trading
        if: (steps.exec_mode.outputs.mode == 'grok_daily' || steps.exec_mode.outputs.mode == 'grok_forced') && github.event.inputs.skip_trading_day_check != 'true'
        env:
          JQUANTS_API_KEY: ${{ secrets.JQUANTS_API_KEY }}
        run: |
          echo "ðŸ” Checking if next day is a trading day..."
          python scripts/check_next_day_trading.py
          EXIT_CODE=$?

          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… Next day is a TRADING day - will execute Grok selection"
          else
            echo "âŒ Next day is NOT a trading day - skipping Grok selection"
            echo "::notice::Skipping Grok selection - next day is not a trading day"
          fi

          exit $EXIT_CODE

      - name: Skip if outside trading window
        if: |
          steps.check_updated.outputs.already_updated == 'true'
        run: |
          echo "::notice::Skipping pipeline execution - already updated today"
          exit 0

      - name: Verify S3 backups before Grok selection
        id: verify_backup
        if: success() && steps.exec_mode.outputs.mode == 'grok_daily' && github.event_name != 'workflow_dispatch'
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "ðŸ” Verifying S3 backups before Grok selection..."
          echo "============================================================"

          # verify_grok_backup.py ã¯ --date ã‚’çœç•¥ã™ã‚‹ã¨
          # è‡ªå‹•çš„ã« grok_trending.parquet ã‹ã‚‰æ—¥ä»˜ã‚’èª­ã¿å–ã‚‹
          python3 scripts/verify_grok_backup.py --bucket "$S3_BUCKET"
          EXIT_CODE=$?

          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… S3 backups verified successfully"
            echo "backup_verified=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ S3 backup verification failed"
            echo "âš ï¸  Aborting Grok selection to prevent data loss"
            exit 1
          fi

      - name: Cleanup grok_trending before selection
        if: success() && steps.exec_mode.outputs.mode == 'grok_daily' && github.event_name != 'workflow_dispatch'
        run: |
          echo "ðŸ§¹ Cleaning up grok_trending.parquet on S3..."
          echo "============================================================"

          # cleanup_grok_trending.py ãŒS3ã‹ã‚‰èª­ã¿è¾¼ã¿ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã®ã§ã€ç„¡æ¡ä»¶ã«å®Ÿè¡Œ
          python3 scripts/cleanup_grok_trending.py
          if [ $? -eq 0 ]; then
            echo "âœ… Cleanup completed successfully"
          else
            echo "âŒ Cleanup failed"
            exit 1
          fi

      - name: Run Data Pipeline
        id: pipeline
        if: |
          success() && (
            steps.exec_mode.outputs.mode == 'stock_data' ||
            steps.exec_mode.outputs.mode == 'stock_data_forced' ||
            steps.check_next_trading.conclusion == 'success' ||
            steps.check_next_trading.conclusion == 'skipped'
          )
        env:
          JQUANTS_API_KEY: ${{ secrets.JQUANTS_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          FORCE_META_UPDATE: ${{ steps.check_trading.outputs.force_meta_update }}
          SKIP_GROK_GENERATION: ${{ steps.exec_mode.outputs.skip_grok }}
          PROMPT_VERSION: v1_2_increased_selection
          ALLOW_GROK_API: "1"
        run: |
          # Create .env.xai for Grok API
          echo "XAI_API_KEY=$XAI_API_KEY" > .env.xai

          echo "============================================================"
          echo "Data Pipeline Execution (GitHub Actions)"
          echo "Started at: $(TZ=Asia/Tokyo date +"%Y-%m-%d %H:%M:%S JST")"
          echo "============================================================"
          echo ""
          echo "Environment variables:"
          echo "  S3_BUCKET=${{ env.S3_BUCKET }}"
          echo "  PARQUET_PREFIX=${{ env.S3_PREFIX }}"
          echo "  AWS_REGION=${{ env.AWS_REGION }}"
          echo "  FORCE_META_UPDATE=$FORCE_META_UPDATE"
          echo "  SKIP_GROK_GENERATION=$SKIP_GROK_GENERATION"
          echo "  LATEST_TRADING_DAY=${{ steps.check_trading.outputs.latest_trading_day }}"
          echo ""

          # Force update flag for meta_jquants (manual trigger override)
          if [ "${{ github.event.inputs.force_meta_jquants }}" = "true" ]; then
            export FORCE_META_UPDATE=true
            echo "âš ï¸ Manual force update enabled for meta_jquants.parquet"
          fi

          # å–å¼•åˆ¶é™ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ23:00 Groké¸å®šç”¨ï¼‰
          echo "ðŸ“¥ Downloading trading restriction data for Grok selection..."
          mkdir -p data/parquet

          # 1. margin_code_master.parquetï¼ˆS3ã‹ã‚‰ï¼‰
          if ! aws s3 cp "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}margin_code_master.parquet" "data/parquet/margin_code_master.parquet" --only-show-errors; then
            echo "âŒ ERROR: Failed to download margin_code_master.parquet from S3"
            exit 1
          fi
          echo "âœ… margin_code_master.parquet downloaded from S3"

          # 2. jsf_seigenichiran.csvï¼ˆæ—¥è¨¼é‡‘ã‚µã‚¤ãƒˆâ†’S3ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
          JSF_URL="https://www.taisyaku.jp/data/seigenichiran.csv"
          JSF_PATH="data/parquet/jsf_seigenichiran.csv"
          echo "ðŸ“¥ Downloading JSF restrictions from $JSF_URL..."
          if curl -fsSL --connect-timeout 10 --max-time 30 "$JSF_URL" -o "$JSF_PATH" 2>/dev/null; then
            # Shift-JIS â†’ UTF-8 å¤‰æ›
            if command -v iconv &> /dev/null; then
              iconv -f SHIFT-JIS -t UTF-8 "$JSF_PATH" > "${JSF_PATH}.tmp" && mv "${JSF_PATH}.tmp" "$JSF_PATH"
            fi
            echo "âœ… JSF restrictions downloaded from æ—¥è¨¼é‡‘ (latest)"
          else
            echo "âš ï¸ Failed to download from æ—¥è¨¼é‡‘, using S3 fallback..."
            if ! aws s3 cp "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}jsf_seigenichiran.csv" "$JSF_PATH" --only-show-errors; then
              echo "âŒ ERROR: Failed to download jsf_seigenichiran.csv from S3 fallback"
              exit 1
            fi
            echo "âœ… JSF restrictions downloaded from S3 (fallback)"
          fi

          # Run the NEW pipeline (scalping skip + Grok)
          # Old pipeline (with scalping): python scripts/run_pipeline.py
          python scripts/run_pipeline_scalping_skip_add_grok.py
          EXIT_CODE=$?

          echo ""
          echo "============================================================"
          echo "Pipeline completed with exit code: $EXIT_CODE"
          echo "============================================================"

          exit $EXIT_CODE

      - name: Generate trading recommendation (23:00 only)
        if: success() && steps.exec_mode.outputs.skip_grok == 'false'
        env:
          INPUT_GROK_TRENDING: data/parquet/grok_trending.parquet
          OUTPUT_RECOMMENDATION_JSON: data/parquet/backtest/trading_recommendation.json
        run: |
          echo "============================================================"
          echo "Generating trading_recommendation.json (23:00 Grok selection)"
          echo "============================================================"

          mkdir -p data/parquet/backtest

          # å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
          echo "ðŸ” Checking required files..."
          if [ ! -f "data/parquet/prices_max_1d.parquet" ]; then
            echo "âŒ ERROR: prices_max_1d.parquet not found (required for v2.1)"
            echo "This file should have been generated by the pipeline step"
            exit 1
          fi
          echo "âœ… prices_max_1d.parquet exists"

          # S3ã‹ã‚‰grok_trending.parquetã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ23:00ã«ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ï¼‰
          echo "ðŸ“¥ Downloading grok_trending.parquet from S3..."
          if ! aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_trending.parquet" \
            data/parquet/grok_trending.parquet \
            --region ${{ env.AWS_REGION }} \
            --only-show-errors; then
            echo "âŒ ERROR: Failed to download grok_trending.parquet from S3"
            exit 1
          fi
          echo "âœ… Downloaded grok_trending.parquet"

          # æ™‚ä¾¡ç·é¡ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆé¸å®šæ—¥çµ‚å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
          echo "ðŸ“Š Adding market_cap to grok_trending.parquet..."
          python3 scripts/pipeline/add_market_cap_to_grok_trending.py
          if [ $? -eq 0 ]; then
            echo "âœ… Added market_cap column"
            # æ›´æ–°ã•ã‚ŒãŸgrok_trending.parquetã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            aws s3 cp \
              data/parquet/grok_trending.parquet \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_trending.parquet" \
              --region ${{ env.AWS_REGION }} \
              --only-show-errors
            echo "âœ… Uploaded updated grok_trending.parquet to S3"
          else
            echo "âš ï¸ Warning: Failed to add market_cap (continuing anyway)"
          fi

          # MLäºˆæ¸¬ï¼ˆprob_up, quintileï¼‰ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
          echo "ðŸ¤– Adding ML predictions (prob_up, quintile) to grok_trending.parquet..."

          # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          echo "ðŸ“¥ Downloading files for ML prediction..."

          # grok_prices_max_1d.parquet
          if ! aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_prices_max_1d.parquet" \
            data/parquet/grok_prices_max_1d.parquet \
            --only-show-errors; then
            echo "âš ï¸ Warning: grok_prices_max_1d.parquet not found in S3"
          fi

          # grok_trending_archive.parquetï¼ˆgenerate_grok_prices_max_1d.pyç”¨ï¼‰
          aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/grok_trending_archive.parquet" \
            data/parquet/backtest/grok_trending_archive.parquet \
            --only-show-errors || echo "âš ï¸ archive not found"

          # æ–°è¦éŠ˜æŸ„ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆgrok_trendingã®éŠ˜æŸ„ãŒgrok_prices_max_1dã«ãªã„å ´åˆï¼‰
          echo "ðŸ“Š Updating grok_prices_max_1d.parquet with new tickers..."
          python3 scripts/pipeline/generate_grok_prices_max_1d.py || echo "âš ï¸ Price update failed"

          # æ›´æ–°ã•ã‚ŒãŸgrok_prices_max_1d.parquetã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
          if [ -f "data/parquet/grok_prices_max_1d.parquet" ]; then
            aws s3 cp \
              data/parquet/grok_prices_max_1d.parquet \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_prices_max_1d.parquet" \
              --only-show-errors
            echo "âœ… Updated grok_prices_max_1d.parquet on S3"
          fi

          # MLãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
          mkdir -p models
          if ! aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}ml/grok_lgbm_model.pkl" \
            models/grok_lgbm_model.pkl \
            --only-show-errors; then
            echo "âš ï¸ Warning: ML model not found in S3"
          fi
          if ! aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}ml/grok_lgbm_meta.json" \
            models/grok_lgbm_meta.json \
            --only-show-errors; then
            echo "âš ï¸ Warning: ML meta not found in S3"
          fi

          # å¸‚å ´ãƒ‡ãƒ¼ã‚¿ï¼ˆindex, futures, currencyï¼‰
          aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}index_prices_max_1d.parquet" \
            data/parquet/index_prices_max_1d.parquet \
            --only-show-errors || echo "âš ï¸ index_prices not found"
          aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}futures_prices_max_1d.parquet" \
            data/parquet/futures_prices_max_1d.parquet \
            --only-show-errors || echo "âš ï¸ futures_prices not found"
          aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}currency_prices_max_1d.parquet" \
            data/parquet/currency_prices_max_1d.parquet \
            --only-show-errors || echo "âš ï¸ currency_prices not found"

          # MLäºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
          python3 scripts/pipeline/add_ml_prediction_to_grok_trending.py
          if [ $? -eq 0 ]; then
            echo "âœ… Added prob_up/quintile columns"
            # æ›´æ–°ã•ã‚ŒãŸgrok_trending.parquetã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            aws s3 cp \
              data/parquet/grok_trending.parquet \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_trending.parquet" \
              --region ${{ env.AWS_REGION }} \
              --only-show-errors
            echo "âœ… Uploaded updated grok_trending.parquet (with ML predictions) to S3"
          else
            echo "âš ï¸ Warning: Failed to add ML predictions (continuing anyway)"
          fi

          # trading_recommendation.json ã‚’ç”Ÿæˆï¼ˆv2.1ãƒ­ã‚¸ãƒƒã‚¯ - 2éšŽå»ºã¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰
          echo "ðŸ“Š Generating trading_recommendation.json (v2.1 - 2éšŽå»ºã¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)..."
          python3 scripts/pipeline/generate_trading_recommendation_v2_1.py

          if [ $? -eq 0 ] && [ -f "data/parquet/backtest/trading_recommendation.json" ]; then
            echo "âœ… Generated trading_recommendation.json"

            # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            echo "ðŸ“¤ Uploading trading_recommendation.json to S3..."
            aws s3 cp \
              data/parquet/backtest/trading_recommendation.json \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/trading_recommendation.json" \
              --region ${{ env.AWS_REGION }} \
              --only-show-errors
            echo "âœ… Uploaded trading_recommendation.json to S3"
          else
            echo "âŒ ERROR: Failed to generate trading_recommendation.json"
            exit 1
          fi

          echo "============================================================"

      - name: Update day trade list (23:00 only)
        if: success() && steps.exec_mode.outputs.skip_grok == 'false'
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
          S3_PREFIX: ${{ env.S3_PREFIX }}
        run: |
          echo "============================================================"
          echo "Updating grok_day_trade_list.parquet (23:00 Grok selection)"
          echo "============================================================"

          # grok_trending.parquet ã¯å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿
          if [ ! -f "data/parquet/grok_trending.parquet" ]; then
            echo "âŒ ERROR: grok_trending.parquet not found"
            exit 1
          fi

          # éŠ˜æŸ„è¿½åŠ ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
          # - S3ã‹ã‚‰ grok_day_trade_list.parquet ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          # - æ–°è¦éŠ˜æŸ„ã®ã¿è¿½åŠ ï¼ˆä¸Šæ›¸ãåŽ³ç¦ï¼‰
          # - S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
          python3 scripts/pipeline/update_day_trade_list.py

          if [ $? -eq 0 ]; then
            echo "âœ… Day trade list updated successfully"
          else
            echo "âš ï¸ Day trade list update failed (non-critical)"
          fi

          echo "============================================================"

      - name: Run backtest and archive
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        run: |
          echo "============================================================"
          echo "Running backtest and archiving results (16:45 only)"
          echo "============================================================"

          # S3ã‹ã‚‰æ—¢å­˜ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          mkdir -p data/parquet/backtest
          ARCHIVE_FILE="grok_trending_archive.parquet"
          S3_ARCHIVE_PATH="s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$ARCHIVE_FILE"

          if aws s3 ls "$S3_ARCHIVE_PATH" > /dev/null 2>&1; then
            echo "ðŸ“¥ Downloading existing archive from S3..."
            aws s3 cp "$S3_ARCHIVE_PATH" "data/parquet/backtest/$ARCHIVE_FILE" --only-show-errors
            EXISTING_RECORDS=$(python -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/$ARCHIVE_FILE'); print(len(df))")
            EXISTING_DATES=$(python -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/$ARCHIVE_FILE'); print(df['backtest_date'].nunique())")
            echo "âœ… Downloaded archive: $EXISTING_RECORDS records across $EXISTING_DATES days"
          else
            echo "â„¹ï¸  No existing archive found in S3, will create new one"
          fi

          # å–å¼•åˆ¶é™ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆmargin_code + æ—¥è¨¼é‡‘åˆ¶é™ï¼‰- å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«
          echo "ðŸ“¥ Downloading trading restriction data (required)..."

          # 1. margin_code_master.parquetï¼ˆS3ã‹ã‚‰ï¼‰
          if ! aws s3 cp "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}margin_code_master.parquet" "data/parquet/margin_code_master.parquet" --only-show-errors; then
            echo "âŒ ERROR: Failed to download margin_code_master.parquet from S3"
            exit 1
          fi
          echo "âœ… margin_code_master.parquet downloaded from S3"

          # 2. jsf_seigenichiran.csvï¼ˆæ—¥è¨¼é‡‘ã‚µã‚¤ãƒˆâ†’S3ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
          JSF_URL="https://www.taisyaku.jp/data/seigenichiran.csv"
          JSF_PATH="data/parquet/jsf_seigenichiran.csv"
          echo "ðŸ“¥ Downloading JSF restrictions from $JSF_URL..."
          if curl -fsSL --connect-timeout 10 --max-time 30 "$JSF_URL" -o "$JSF_PATH" 2>/dev/null; then
            # Shift-JIS â†’ UTF-8 å¤‰æ›
            if command -v iconv &> /dev/null; then
              iconv -f SHIFT-JIS -t UTF-8 "$JSF_PATH" > "${JSF_PATH}.tmp" && mv "${JSF_PATH}.tmp" "$JSF_PATH"
            fi
            echo "âœ… JSF restrictions downloaded from æ—¥è¨¼é‡‘ (latest)"
          else
            echo "âš ï¸ Failed to download from æ—¥è¨¼é‡‘, using S3 fallback..."
            if ! aws s3 cp "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}jsf_seigenichiran.csv" "$JSF_PATH" --only-show-errors; then
              echo "âŒ ERROR: Failed to download jsf_seigenichiran.csv from S3 fallback"
              exit 1
            fi
            echo "âœ… JSF restrictions downloaded from S3 (fallback)"
          fi

          # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆS3ã®GROKé¸å®šçµæžœã‚’ä½¿ç”¨ï¼‰
          echo "Running backtest from S3 grok_trending.parquet..."
          python scripts/pipeline/save_backtest_to_archive.py

          if [ $? -eq 0 ]; then
            echo "âœ… Backtest completed and archived"
          else
            echo "âš ï¸ Backtest failed (non-critical)"
          fi

          echo "============================================================"

      - name: Extract data statistics
        id: stats
        if: success()
        run: |
          # meta_jquants statistics
          META_JQUANTS_COUNT=0
          if [ -f "data/parquet/meta_jquants.parquet" ]; then
            META_JQUANTS_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/meta_jquants.parquet')))")
          fi
          echo "meta_jquants_count=$META_JQUANTS_COUNT" >> $GITHUB_OUTPUT

          # grok_trending statistics (from S3)
          GROK_COUNT=0
          if aws s3 ls "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_trending.parquet" > /dev/null 2>&1; then
            mkdir -p data/parquet
            aws s3 cp "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_trending.parquet" data/parquet/grok_trending.parquet --only-show-errors
            GROK_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/grok_trending.parquet')))")
            rm -f data/parquet/grok_trending.parquet
          fi
          echo "grok_trending_count=$GROK_COUNT" >> $GITHUB_OUTPUT

          # grok_backtest_meta statistics (NEW)
          GROK_BACKTEST_EXISTS=false
          if [ -f "data/parquet/grok_backtest_meta.parquet" ]; then
            GROK_BACKTEST_EXISTS=true
          fi
          echo "grok_backtest_exists=$GROK_BACKTEST_EXISTS" >> $GITHUB_OUTPUT

          # grok_trending_archive statistics (NEW)
          ARCHIVE_TOTAL=0
          ARCHIVE_DATES=0
          if [ -f "data/parquet/backtest/grok_trending_archive.parquet" ]; then
            ARCHIVE_TOTAL=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/backtest/grok_trending_archive.parquet')))")
            ARCHIVE_DATES=$(python -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/grok_trending_archive.parquet'); print(df['backtest_date'].nunique())")
          fi
          echo "archive_total=$ARCHIVE_TOTAL" >> $GITHUB_OUTPUT
          echo "archive_dates=$ARCHIVE_DATES" >> $GITHUB_OUTPUT

          # scalping statistics (kept for compatibility, will be 0)
          ENTRY_COUNT=0
          ACTIVE_COUNT=0
          if [ -f "data/parquet/scalping_entry.parquet" ]; then
            ENTRY_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/scalping_entry.parquet')))")
          fi
          if [ -f "data/parquet/scalping_active.parquet" ]; then
            ACTIVE_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/scalping_active.parquet')))")
          fi
          echo "scalping_entry_count=$ENTRY_COUNT" >> $GITHUB_OUTPUT
          echo "scalping_active_count=$ACTIVE_COUNT" >> $GITHUB_OUTPUT

          # all_stocks statistics
          ALL_STOCKS_COUNT=0
          if [ -f "data/parquet/all_stocks.parquet" ]; then
            ALL_STOCKS_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/all_stocks.parquet')))")
          fi
          echo "all_stocks_count=$ALL_STOCKS_COUNT" >> $GITHUB_OUTPUT

          # prices statistics
          LATEST_DATE="N/A"
          if [ -f "data/parquet/prices_max_1d.parquet" ]; then
            LATEST_DATE=$(python -c "import pandas as pd; df=pd.read_parquet('data/parquet/prices_max_1d.parquet'); print(df['date'].max() if not df.empty else 'N/A')")
          fi
          echo "latest_price_date=$LATEST_DATE" >> $GITHUB_OUTPUT

          echo "ðŸ“Š Statistics extracted:"
          echo "  - meta_jquants: $META_JQUANTS_COUNT stocks"
          echo "  - grok_trending: $GROK_COUNT stocks"
          echo "  - grok_archive: $ARCHIVE_TOTAL records across $ARCHIVE_DATES days"
          echo "  - scalping_entry: $ENTRY_COUNT stocks (skipped)"
          echo "  - scalping_active: $ACTIVE_COUNT stocks (skipped)"
          echo "  - all_stocks: $ALL_STOCKS_COUNT stocks"
          echo "  - latest_price_date: $LATEST_DATE"

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-output-${{ github.run_id }}
          path: |
            data/parquet/*.parquet
            data/parquet/manifest.json
          if-no-files-found: warn
          retention-days: 7

      - name: Send Slack notification (skipped 02:00 JST)
        if: |
          github.event.schedule == '0 17 * * *' &&
          steps.check_updated.outputs.already_updated == 'true'
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_INCOMING_WEBHOOK_URL }}
        run: |
          TODAY=$(TZ=Asia/Tokyo date +%Y-%m-%d)
          MESSAGE='{
            "text": "â„¹ï¸ Data Pipeline Skipped (Already Updated)",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "â„¹ï¸ *Data Pipeline Skipped*\nData already updated today (`'"${TODAY}"'`). Skipping 02:00 JST run."
                }
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*Branch:*\n`${{ github.ref_name }}`"},
                  {"type": "mrkdwn", "text": "*Trigger:*\n`02:00 JST scheduled run`"}
                ]
              },
              {
                "type": "actions",
                "elements": [
                  {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "View Workflow Run"},
                    "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }
            ]
          }'
          curl -X POST -H 'Content-type: application/json' --data "$MESSAGE" "$SLACK_WEBHOOK_URL" || true

      - name: Archive GROK trending for backtest
        id: archive-grok
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        run: |
          # save_backtest_to_archive.py ãŒ grok_trending_{DATE}.parquet ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€
          # ã“ã“ã§ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¯ä¸è¦ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚
          echo "â„¹ï¸  Archiving is handled by save_backtest_to_archive.py"

          if false; then
            # cronåˆ¤å®š
            if [ "${{ github.event.schedule }}" = "45 7 * * *" ]; then
              RUN_TIME="16:45"
            elif [ "${{ github.event.schedule }}" = "0 14 * * *" ]; then
              RUN_TIME="23:00"
            else
              RUN_TIME="manual"
            fi

            echo "ðŸ“¦ Archiving GROK trending for backtest ($RUN_TIME run)..."

            # æ—¥ä»˜å–å¾—ï¼ˆJSTåŸºæº–ã€YYYYMMDDå½¢å¼ï¼‰
            DATE=$(TZ=Asia/Tokyo date +%Y%m%d)
            BACKTEST_FILE="grok_trending_${DATE}.parquet"
            BACKTEST_DIR="data/parquet/backtest"

            echo "[INFO] Run time: $RUN_TIME"
            echo "[INFO] JST Date: $DATE"
            echo "[INFO] Backup file: $BACKTEST_FILE (will overwrite if exists)"

            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            mkdir -p "$BACKTEST_DIR"

            # ã‚³ãƒ”ãƒ¼
            cp "data/parquet/grok_trending.parquet" "$BACKTEST_DIR/$BACKTEST_FILE"
            echo "âœ… Created: $BACKTEST_DIR/$BACKTEST_FILE"

            # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸Šæ›¸ãä¿å­˜ï¼‰
            aws s3 cp "$BACKTEST_DIR/$BACKTEST_FILE" \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$BACKTEST_FILE" \
              --region ${{ env.AWS_REGION }}
            echo "âœ… Uploaded to S3: s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$BACKTEST_FILE"

            echo "âœ… Backtest archiving completed"
          else
            echo "âš ï¸  grok_trending.parquet not found, skipping backtest archive"
          fi

      - name: Generate Granville IFD signals
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        run: |
          echo "============================================================"
          echo "Generating Granville IFD signals (16:45 only)"
          echo "============================================================"

          # CIå…ˆè¡ŒæŒ‡æ•°ãƒ‡ãƒ¼ã‚¿ï¼ˆimprovement/data/macro/ï¼‰ãŒãƒªãƒã‚¸ãƒˆãƒªã«ãªã„å ´åˆã¯S3ã‹ã‚‰å–å¾—
          MACRO_DIR="improvement/data/macro"
          CI_FILE="$MACRO_DIR/estat_ci_index.parquet"
          if [ ! -f "$CI_FILE" ]; then
            echo "ðŸ“¥ Downloading CI index from S3..."
            mkdir -p "$MACRO_DIR"
            aws s3 cp \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}macro/estat_ci_index.parquet" \
              "$CI_FILE" --only-show-errors 2>/dev/null || true
          fi

          # S3ã‹ã‚‰æ—¢å­˜ã‚·ã‚°ãƒŠãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆè“„ç©ç”¨ï¼‰
          SIGNALS_FILE="data/parquet/granville_ifd_signals.parquet"
          if aws s3 ls "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}granville_ifd_signals.parquet" > /dev/null 2>&1; then
            echo "ðŸ“¥ Downloading existing signals from S3..."
            aws s3 cp \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}granville_ifd_signals.parquet" \
              "$SIGNALS_FILE" --only-show-errors
            EXISTING_COUNT=$(python3 -c "import pandas as pd; print(len(pd.read_parquet('$SIGNALS_FILE')))")
            echo "âœ… Downloaded existing signals: $EXISTING_COUNT rows"
          fi

          python3 scripts/pipeline/generate_granville_signals.py

          if [ $? -eq 0 ]; then
            echo "âœ… Granville signal generation completed"
            # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            aws s3 cp "$SIGNALS_FILE" \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}granville_ifd_signals.parquet" \
              --only-show-errors
            echo "âœ… Uploaded granville_ifd_signals.parquet to S3"
          else
            echo "âš ï¸ Granville signal generation failed (non-critical)"
          fi

      - name: Backtest Granville IFD archive
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        run: |
          echo "============================================================"
          echo "Backtesting Granville IFD archive (16:45 only)"
          echo "============================================================"

          python3 scripts/pipeline/backtest_granville_ifd.py

          if [ $? -eq 0 ]; then
            echo "âœ… Granville backtest completed"
          else
            echo "âš ï¸ Granville backtest failed (non-critical)"
          fi

      - name: Format Granville Slack notification
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        run: |
          python3 scripts/format_granville_slack.py || true

      - name: ML Model Retraining (Friday only)
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        run: |
          # é‡‘æ›œæ—¥ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆJSTåŸºæº–ï¼‰
          DAY_OF_WEEK=$(TZ=Asia/Tokyo date +%u)  # 1=æœˆæ›œ, 5=é‡‘æ›œ

          if [ "$DAY_OF_WEEK" != "5" ]; then
            echo "â„¹ï¸  Today is not Friday (day=$DAY_OF_WEEK). Skipping ML retraining."
            exit 0
          fi

          echo "============================================================"
          echo "ðŸ¤– ML Model Retraining (Friday 16:45)"
          echo "============================================================"

          # === Step 1: å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ===
          echo ""
          echo "ðŸ“¥ Step 1: Downloading required data from S3..."

          # grok_trending_archive.parquet
          aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/grok_trending_archive.parquet" \
            data/parquet/backtest/grok_trending_archive.parquet \
            --only-show-errors
          echo "  âœ… grok_trending_archive.parquet"

          # grok_prices_max_1d.parquetï¼ˆS3ã«å­˜åœ¨ã—ãªã„å ´åˆã¯Step 2ã§å†ç”Ÿæˆï¼‰
          if aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_prices_max_1d.parquet" \
            data/parquet/grok_prices_max_1d.parquet \
            --only-show-errors 2>/dev/null; then
            echo "  âœ… grok_prices_max_1d.parquet"
          else
            echo "  âš ï¸ grok_prices_max_1d.parquet not found in S3 (will be generated in Step 2)"
          fi

          # index_prices_max_1d.parquet
          aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}index_prices_max_1d.parquet" \
            data/parquet/index_prices_max_1d.parquet \
            --only-show-errors
          echo "  âœ… index_prices_max_1d.parquet"

          # futures_prices_max_1d.parquet
          aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}futures_prices_max_1d.parquet" \
            data/parquet/futures_prices_max_1d.parquet \
            --only-show-errors
          echo "  âœ… futures_prices_max_1d.parquet"

          # currency_prices_max_1d.parquet
          aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}currency_prices_max_1d.parquet" \
            data/parquet/currency_prices_max_1d.parquet \
            --only-show-errors
          echo "  âœ… currency_prices_max_1d.parquet"

          # === Step 2: grok_prices_max_1d.parquet ã‚’æ›´æ–° ===
          echo ""
          echo "ðŸ“Š Step 2: Updating grok_prices_max_1d.parquet..."
          python3 scripts/pipeline/generate_grok_prices_max_1d.py

          if [ $? -eq 0 ]; then
            echo "  âœ… grok_prices_max_1d.parquet updated"
            # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            aws s3 cp \
              data/parquet/grok_prices_max_1d.parquet \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_prices_max_1d.parquet" \
              --only-show-errors
            echo "  âœ… Uploaded to S3"
          else
            echo "  âš ï¸ Failed to update grok_prices_max_1d.parquet (continuing with existing data)"
          fi

          # === Step 3: ç‰¹å¾´é‡ç”Ÿæˆ ===
          echo ""
          echo "ðŸ“Š Step 3: Generating features..."
          mkdir -p data/parquet/ml
          python3 scripts/ml/feature_engineering.py

          if [ $? -eq 0 ]; then
            FEATURES_COUNT=$(python3 -c "import pandas as pd; df=pd.read_parquet('data/parquet/ml/archive_with_features.parquet'); print(len(df))")
            echo "  âœ… Generated features: $FEATURES_COUNT records"
          else
            echo "  âŒ Failed to generate features"
            exit 1
          fi

          # === Step 4: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ===
          echo ""
          echo "ðŸ¤– Step 4: Training ML model..."
          mkdir -p models
          python3 scripts/ml/train_model.py

          if [ $? -eq 0 ]; then
            echo "  âœ… Model training completed"
          else
            echo "  âŒ Failed to train model"
            exit 1
          fi

          # === Step 5: ãƒ¢ãƒ‡ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ===
          echo ""
          echo "ðŸ“¤ Step 5: Uploading model to S3..."

          aws s3 cp \
            models/grok_lgbm_model.pkl \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}ml/grok_lgbm_model.pkl" \
            --only-show-errors
          echo "  âœ… grok_lgbm_model.pkl"

          aws s3 cp \
            models/grok_lgbm_meta.json \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}ml/grok_lgbm_meta.json" \
            --only-show-errors
          echo "  âœ… grok_lgbm_meta.json"

          # ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
          aws s3 cp \
            data/parquet/ml/archive_with_features.parquet \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}ml/archive_with_features.parquet" \
            --only-show-errors
          echo "  âœ… archive_with_features.parquet"

          echo ""
          echo "============================================================"
          echo "âœ… ML Model Retraining completed!"
          echo "============================================================"

      - name: Generate Grok Analysis Data
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        run: |
          echo "ðŸ“Š Generating Grok Analysis Data..."

          # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
          mkdir -p data/parquet/backtest

          # === Step 1: S3ã‹ã‚‰å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ===
          echo "ðŸ“¥ Downloading files from S3..."

          # grok_trending_*.parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          echo "  - Downloading grok_trending_*.parquet files..."
          aws s3 sync \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/" \
            data/parquet/backtest/ \
            --exclude "*" \
            --include "grok_trending_202*.parquet" \
            --only-show-errors

          GROK_FILES_COUNT=$(ls data/parquet/backtest/grok_trending_202*.parquet 2>/dev/null | wc -l)
          echo "  âœ… Downloaded $GROK_FILES_COUNT grok_trending files"

          # trading_recommendation.json ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ23:00ã«ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ï¼‰
          echo "  - Downloading trading_recommendation.json (required)..."
          if ! aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/trading_recommendation.json" \
            data/parquet/backtest/trading_recommendation.json \
            --only-show-errors; then
            echo "  âŒ ERROR: trading_recommendation.json not found in S3"
            echo "  This file is required for merge_json_to_grok_analysis.py"
            echo "  Pipeline cannot proceed without this file"
            exit 1
          fi
          echo "  âœ… Downloaded trading_recommendation.json"

          # grok_trending.parquet (æœ€æ–°ã®é¸å®šçµæžœ) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ - å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«
          echo "  - Downloading grok_trending.parquet (required)..."
          if ! aws s3 cp \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_trending.parquet" \
            data/parquet/grok_trending.parquet \
            --only-show-errors; then
            echo "  âŒ ERROR: Failed to download grok_trending.parquet from S3"
            echo "  This file should have been created by yesterday's 23:00 Grok selection"
            echo "  Pipeline cannot proceed without this file"
            exit 1
          fi
          echo "  âœ… Downloaded grok_trending.parquet"

          # grok_trending.parquetã‚’backtest/ã«ã‚‚ã‚³ãƒ”ãƒ¼ï¼ˆv2.1ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ï¼‰
          echo "  ðŸ“‹ Copying grok_trending.parquet to backtest/ directory..."
          cp data/parquet/grok_trending.parquet data/parquet/backtest/grok_trending.parquet
          echo "  âœ… Copied to backtest/"

          # === Step 2: grok_analysis_merged.parquet ã®ç”Ÿæˆ ===
          echo ""
          echo "ðŸ“Š Step 2: Generating grok_analysis_merged.parquet..."
          echo "  â„¹ï¸  Using merge_json_to_grok_analysis.py (49 columns with v2.0.3 logic)"

          # S3ã‹ã‚‰æ—¢å­˜ã®grok_analysis_merged.parquetã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          ANALYSIS_FILE="grok_analysis_merged.parquet"
          S3_ANALYSIS_PATH="s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$ANALYSIS_FILE"

          if aws s3 ls "$S3_ANALYSIS_PATH" > /dev/null 2>&1; then
            echo "  ðŸ“¥ Downloading existing grok_analysis_merged.parquet from S3..."
            aws s3 cp "$S3_ANALYSIS_PATH" "data/parquet/backtest/$ANALYSIS_FILE" --only-show-errors
            EXISTING_RECORDS=$(python3 -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/$ANALYSIS_FILE'); print(len(df))")
            EXISTING_DATES=$(python3 -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/$ANALYSIS_FILE'); print(df['backtest_date'].nunique())")
            echo "  âœ… Downloaded existing file: $EXISTING_RECORDS records across $EXISTING_DATES days"
          else
            echo "  â„¹ï¸  No existing grok_analysis_merged.parquet found in S3, will create new one"
          fi

          python3 scripts/pipeline/merge_json_to_grok_analysis.py

          if [ $? -eq 0 ]; then
            RECORDS=$(python3 -c "import pandas as pd; df = pd.read_parquet('data/parquet/backtest/grok_analysis_merged.parquet'); print(len(df))")
            COLUMNS=$(python3 -c "import pandas as pd; df = pd.read_parquet('data/parquet/backtest/grok_analysis_merged.parquet'); print(len(df.columns))")
            echo "  âœ… Generated grok_analysis_merged.parquet ($RECORDS records, $COLUMNS columns)"
          else
            echo "  âŒ Failed to run merge_json_to_grok_analysis.py"
            exit 1
          fi

          # === Step 2.5: grok_analysis_merged_v2_1.parquet ã®ç”Ÿæˆ ===
          echo ""
          echo "ðŸ“Š Step 2.5: Generating grok_analysis_merged_v2_1.parquet..."
          echo "  â„¹ï¸  Using update_grok_analysis_merged_v2_1.py (61 columns with v2.1 logic)"

          # S3ã‹ã‚‰æ—¢å­˜ã®grok_analysis_merged_v2_1.parquetã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          ANALYSIS_V2_1_FILE="grok_analysis_merged_v2_1.parquet"
          S3_ANALYSIS_V2_1_PATH="s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$ANALYSIS_V2_1_FILE"

          if aws s3 ls "$S3_ANALYSIS_V2_1_PATH" > /dev/null 2>&1; then
            echo "  ðŸ“¥ Downloading existing grok_analysis_merged_v2_1.parquet from S3..."
            aws s3 cp "$S3_ANALYSIS_V2_1_PATH" "data/parquet/backtest/$ANALYSIS_V2_1_FILE" --only-show-errors
            EXISTING_RECORDS_V2_1=$(python3 -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/$ANALYSIS_V2_1_FILE'); print(len(df))")
            EXISTING_DATES_V2_1=$(python3 -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/$ANALYSIS_V2_1_FILE'); print(df['backtest_date'].nunique())")
            echo "  âœ… Downloaded existing file: $EXISTING_RECORDS_V2_1 records across $EXISTING_DATES_V2_1 days"
          else
            echo "  â„¹ï¸  No existing grok_analysis_merged_v2_1.parquet found in S3, will create new one"
          fi

          python3 scripts/pipeline/update_grok_analysis_merged_v2_1.py

          if [ $? -eq 0 ]; then
            RECORDS_V2_1=$(python3 -c "import pandas as pd; df = pd.read_parquet('data/parquet/backtest/grok_analysis_merged_v2_1.parquet'); print(len(df))")
            COLUMNS_V2_1=$(python3 -c "import pandas as pd; df = pd.read_parquet('data/parquet/backtest/grok_analysis_merged_v2_1.parquet'); print(len(df.columns))")
            echo "  âœ… Generated grok_analysis_merged_v2_1.parquet ($RECORDS_V2_1 records, $COLUMNS_V2_1 columns)"
          else
            echo "  âŒ Failed to run update_grok_analysis_merged_v2_1.py"
            exit 1
          fi

          # === Step 3: S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ===
          echo ""
          echo "ðŸ“¤ Step 3: Uploading to S3..."

          # v2.0.3ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
          if aws s3 cp \
            data/parquet/backtest/grok_analysis_merged.parquet \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/grok_analysis_merged.parquet" \
            --region ${{ env.AWS_REGION }} \
            --only-show-errors; then
            echo "  âœ… Uploaded grok_analysis_merged.parquet to S3"
          else
            echo "  âŒ ERROR: Failed to upload grok_analysis_merged.parquet to S3"
            exit 1
          fi

          # v2.1ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
          if aws s3 cp \
            data/parquet/backtest/grok_analysis_merged_v2_1.parquet \
            "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/grok_analysis_merged_v2_1.parquet" \
            --region ${{ env.AWS_REGION }} \
            --only-show-errors; then
            echo "  âœ… Uploaded grok_analysis_merged_v2_1.parquet to S3"
          else
            echo "  âŒ ERROR: Failed to upload grok_analysis_merged_v2_1.parquet to S3"
            exit 1
          fi

          # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
          FILE_SIZE=$(du -h data/parquet/backtest/grok_analysis_merged.parquet | cut -f1)
          echo ""
          echo "âœ… Grok Analysis Data generation completed"
          echo "   File size: $FILE_SIZE"

      - name: Check xAI billing
        if: success()
        env:
          XAI_MANAGEMENT_API_KEY: ${{ secrets.XAI_MANAGEMENT_API_KEY }}
          XAI_TEAM_ID: ${{ secrets.XAI_TEAM_ID }}
        run: |
          echo "ðŸ’° Checking xAI billing..."

          # Install requests if not already installed
          pip install requests 2>/dev/null || true

          # Run billing check script (outputs to /tmp/billing_section.txt)
          python3 scripts/check_xai_billing.py

          if [ $? -eq 0 ]; then
            echo "  âœ… xAI billing check completed"
          else
            echo "  âš ï¸  xAI billing check failed (non-critical)"
          fi

      - name: Extract GROK stocks info
        id: grok-info
        if: success()
        run: |
          # GROKéŠ˜æŸ„æƒ…å ±ã‚’æŠ½å‡ºï¼ˆS3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
          if aws s3 ls "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_trending.parquet" > /dev/null 2>&1; then
            mkdir -p data/parquet
            aws s3 cp "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}grok_trending.parquet" data/parquet/grok_trending.parquet --only-show-errors

            GROK_INFO=$(python3 -c '
          import pandas as pd
          import json

          df = pd.read_parquet("data/parquet/grok_trending.parquet")

          if df.empty:
              print("")
              exit(0)

          # selected_timeåˆ¥ã®é›†è¨ˆ
          time_counts = df["selected_time"].value_counts().to_dict() if "selected_time" in df.columns else {}

          # éŠ˜æŸ„ãƒªã‚¹ãƒˆç”Ÿæˆï¼ˆå…¨éŠ˜æŸ„ï¼‰
          stocks_list = []
          for _, row in df.iterrows():
              ticker = row.get("ticker", "")
              stock_name = row.get("stock_name", "")
              reason = row.get("reason", "")
              tags = row.get("tags", "")
              selected_time = row.get("selected_time", "")

              # reasonã‚’100æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚
              reason_short = reason[:100] + "..." if len(reason) > 100 else reason

              stocks_list.append({
                  "ticker": ticker,
                  "stock_name": stock_name,
                  "reason": reason_short,
                  "tags": tags,
                  "selected_time": selected_time
              })

          result = {
              "total": len(df),
              "time_counts": time_counts,
              "stocks": stocks_list
          }

          print(json.dumps(result, ensure_ascii=False))
          ' 2>/dev/null)

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            rm -f data/parquet/grok_trending.parquet
          else
            GROK_INFO=""
          fi

          # ãƒžãƒ«ãƒãƒ©ã‚¤ãƒ³å¯¾å¿œ
          echo "grok_info<<EOF" >> $GITHUB_OUTPUT
          echo "$GROK_INFO" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Send Slack notification on success
        if: success()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_INCOMING_WEBHOOK_URL }}
        run: |
          TRIGGER_TIME="Manual Run"
          if [ "${{ github.event_name }}" = "schedule" ]; then
            if [ "${{ github.event.schedule }}" = "0 14 * * *" ]; then
              TRIGGER_TIME="23:00 JST"
            elif [ "${{ github.event.schedule }}" = "0 17 * * *" ]; then
              TRIGGER_TIME="02:00 JST"
            else
              TRIGGER_TIME="16:45 JST"
            fi
          fi

          # J-Quantséšœå®³æ¤œçŸ¥ï¼ˆç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ï¼‰
          JQUANTS_STATUS="âœ… æ­£å¸¸"
          if [ "${{ steps.stats.outputs.meta_jquants_count }}" = "0" ]; then
            JQUANTS_STATUS="âš ï¸ éšœå®³ç™ºç”Ÿï¼ˆé™çš„éŠ˜æŸ„ã®ã¿ã§æ›´æ–°ï¼‰"
          fi

          # GROKéŠ˜æŸ„æƒ…å ±ã‚’ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆ23:00ã®ã¿ï¼‰
          GROK_SECTION=""
          BILLING_SECTION=""

          # xAI billingæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
          XAI_REMAINING=""
          if [ -f "/tmp/xai_remaining.txt" ] && [ -s "/tmp/xai_remaining.txt" ]; then
            XAI_REMAINING=$(cat /tmp/xai_remaining.txt)
          fi
          if [ -f "/tmp/billing_section.txt" ] && [ -s "/tmp/billing_section.txt" ]; then
            BILLING_SECTION=",$(cat /tmp/billing_section.txt)"
          fi

          # Granville IFD (16:45 only)
          GRANVILLE_SECTION=""
          if [ -f "/tmp/granville_section.txt" ] && [ -s "/tmp/granville_section.txt" ]; then
            GRANVILLE_SECTION=$(cat /tmp/granville_section.txt)
          fi

          # 23:00 ã¾ãŸã¯ force_grok=true â†’ GROKéŠ˜æŸ„ã‚’è¡¨ç¤º
          if [ "${{ github.event.schedule }}" = "0 14 * * *" ] || [ "${{ github.event.inputs.force_grok }}" = "true" ]; then
            GROK_INFO='${{ steps.grok-info.outputs.grok_info }}'

            if [ -n "$GROK_INFO" ] && [ "$GROK_INFO" != "null" ]; then
              # åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆYAMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å•é¡Œã‚’å›žé¿ï¼‰
              python3 scripts/format_grok_slack.py "$GROK_INFO"

              # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
              if [ -f "/tmp/grok_section.txt" ] && [ -s "/tmp/grok_section.txt" ]; then
                GROK_SECTION=$(cat /tmp/grok_section.txt)
              fi
            fi
          fi

          # ã‚¿ã‚¤ãƒˆãƒ«æ§‹ç¯‰
          SLACK_TITLE="âœ… Data Pipeline Succeeded"
          if [ -n "$XAI_REMAINING" ]; then
            SLACK_TITLE="${SLACK_TITLE} | xAI: ${XAI_REMAINING}"
          fi

          MESSAGE='{
            "text": "'"${SLACK_TITLE}"'",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "âœ… *Data Pipeline Succeeded*\nWorkflow `${{ github.workflow }}` completed successfully."
                }
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*Branch:*\n`${{ github.ref_name }}`"},
                  {"type": "mrkdwn", "text": "*Trigger:*\n`'"${TRIGGER_TIME}"'`"},
                  {"type": "mrkdwn", "text": "*J-Quants:*\n'"${JQUANTS_STATUS}"'"},
                  {"type": "mrkdwn", "text": "*Latest Price Date:*\n`${{ steps.stats.outputs.latest_price_date }}`"}
                ]
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*meta_jquants:*\n`${{ steps.stats.outputs.meta_jquants_count }}`éŠ˜æŸ„"},
                  {"type": "mrkdwn", "text": "*all_stocks:*\n`${{ steps.stats.outputs.all_stocks_count }}`éŠ˜æŸ„"},
                  {"type": "mrkdwn", "text": "*grok_trending:*\n`${{ steps.stats.outputs.grok_trending_count }}`éŠ˜æŸ„"},
                  {"type": "mrkdwn", "text": "*static_signals:*\n`removed`"}
                ]
              },
              {
                "type": "actions",
                "elements": [
                  {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "View Workflow Run"},
                    "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }'"$BILLING_SECTION$GROK_SECTION$GRANVILLE_SECTION"'
            ]
          }'
          curl -X POST -H 'Content-type: application/json' --data "$MESSAGE" "$SLACK_WEBHOOK_URL" || true

      - name: Send Slack notification on failure
        if: failure()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_INCOMING_WEBHOOK_URL }}
        run: |
          TRIGGER_TIME="Manual Run"
          if [ "${{ github.event_name }}" = "schedule" ]; then
            if [ "${{ github.event.schedule }}" = "0 14 * * *" ]; then
              TRIGGER_TIME="23:00 JST"
            elif [ "${{ github.event.schedule }}" = "0 17 * * *" ]; then
              TRIGGER_TIME="02:00 JST"
            else
              TRIGGER_TIME="16:45 JST"
            fi
          fi

          MESSAGE='{
            "text": "âŒ Data Pipeline Failed",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "âŒ *Data Pipeline Failed*\nWorkflow `${{ github.workflow }}` encountered an error."
                }
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*Branch:*\n`${{ github.ref_name }}`"},
                  {"type": "mrkdwn", "text": "*Trigger:*\n`'"${TRIGGER_TIME}"'`"},
                  {"type": "mrkdwn", "text": "*Event:*\n`${{ github.event_name }}`"}
                ]
              },
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "âš ï¸ *Action Required*\nPlease check the workflow logs and investigate the failure."
                }
              },
              {
                "type": "actions",
                "elements": [
                  {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "View Workflow Run"},
                    "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }
            ]
          }'
          curl -X POST -H 'Content-type: application/json' --data "$MESSAGE" "$SLACK_WEBHOOK_URL" || true

  deploy:
    name: Build and Push to ECR
    needs: pipeline
    if: success()
    runs-on: ubuntu-latest
    environment: AWS_OIDC

    env:
      AWS_REGION: ${{ vars.AWS_REGION || 'ap-northeast-1' }}
      ECR_REPOSITORY: ${{ vars.ECR_REPOSITORY || 'stock-api' }}
      ECR_REPO: ${{ vars.ECR_REPO }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build Docker image with both tags
          docker build -t ${{ env.ECR_REPO }}:$IMAGE_TAG .
          docker build -t ${{ env.ECR_REPO }}:latest .

          # Push both tags to ECR
          docker push ${{ env.ECR_REPO }}:$IMAGE_TAG
          docker push ${{ env.ECR_REPO }}:latest

          echo "image=${{ env.ECR_REPO }}:$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "âœ… Pushed image: ${{ env.ECR_REPO }}:$IMAGE_TAG"
          echo "âœ… Pushed image: ${{ env.ECR_REPO }}:latest"
          echo ""
          echo "ðŸš€ App Runner will automatically deploy the new image."
          echo "ðŸ“¢ Deployment completion notification will be sent via EventBridge â†’ Lambda â†’ Slack"
