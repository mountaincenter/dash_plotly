name: Data Pipeline

on:
  schedule:
    # ÊØéÊó•23ÊôÇÔºàJSTÔºâ„Å´ÂÆüË°åÔºàÂúüÊó•Á•ùÂê´„ÇÄÔºâ
    # GrokÈÅ∏ÂÆö„ÅØÊùêÊñô„ÅÆÈÆÆÂ∫¶„ÅåÈáçË¶Å„Å™„Åü„ÇÅ„ÄÅÊØéÊó•ÂÆüË°å
    # GitHub Actions cron „ÅØÂ∏∏„Å´UTCÊôÇÂàª„ÅßÊåáÂÆö
    # JST = UTC + 9ÊôÇÈñì „ÅÆÂ§âÊèõ„ÇíÂé≥ÂÆà
    - cron: "0 7 * * *" # UTC 07:00 = JST 16:00 (ÂΩìÊó•) - Ê†™‰æ°„Éá„Éº„ÇøÊõ¥Êñ∞„ÅÆ„Åø
    - cron: "0 14 * * *" # UTC 14:00 = JST 23:00 (ÂΩìÊó•) - GrokÈÅ∏ÂÆö + Ê†™‰æ°„Éá„Éº„ÇøÊõ¥Êñ∞
  workflow_dispatch:
    inputs:
      skip_trading_day_check:
        description: "Skip trading day check (force execution regardless of schedule)"
        type: boolean
        default: false
      force_meta_jquants:
        description: "Force update meta_jquants.parquet (ignore 7-day freshness)"
        type: boolean
        default: false
      force_grok:
        description: "Force GROK selection (run generate_grok_trending.py)"
        type: boolean
        default: false

permissions:
  id-token: write
  contents: read

jobs:
  pipeline:
    runs-on: ubuntu-latest
    environment: AWS_OIDC

    env:
      AWS_REGION: ${{ vars.AWS_REGION || 'ap-northeast-1' }}
      S3_BUCKET: ${{ vars.DATA_BUCKET || vars.S3_BUCKET }}
      S3_PREFIX: ${{ vars.PARQUET_PREFIX || 'parquet/' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup J-Quants authentication
        run: |
          # Create .env.jquants for J-Quants API authentication
          cat > .env.jquants <<EOF
          JQUANTS_REFRESH_TOKEN=${{ secrets.JQUANTS_REFRESH_TOKEN }}
          EOF

          echo "‚úÖ J-Quants authentication configured"

      - name: Force execution notice
        if: ${{ github.event.inputs.skip_trading_day_check == 'true' }}
        run: |
          echo "‚ö†Ô∏è  FORCE EXECUTION MODE"
          echo "Trading day check has been bypassed via manual dispatch"
          echo "Pipeline will run regardless of schedule"

      - name: Determine execution mode
        id: exec_mode
        run: |
          CURRENT_HOUR=$(TZ=Asia/Tokyo date +%H)
          echo "Current hour (JST): $CURRENT_HOUR"

          # force_grok „Åå true „ÅÆÂ†¥Âêà„ÄÅÂº∑Âà∂ÁöÑ„Å´GROK„É¢„Éº„Éâ
          if [ "${{ github.event.inputs.force_grok }}" = "true" ]; then
            echo "mode=grok_forced" >> $GITHUB_OUTPUT
            echo "skip_trading_check=true" >> $GITHUB_OUTPUT
            echo "skip_grok=false" >> $GITHUB_OUTPUT
            echo "üöÄ FORCED GROK Mode - Manual GROK selection execution"
          # 23ÊôÇÔºà14:00 UTCÔºâ„ÅÆÂÆüË°å„ÅØGROK„É¢„Éº„ÉâÔºàÂñ∂Ê•≠Êó•„ÉÅ„Çß„ÉÉ„ÇØ„Å™„Åó„ÄÅGrokÈÅ∏ÂÆöÂÆüË°åÔºâ
          elif [ "$CURRENT_HOUR" = "23" ]; then
            echo "mode=grok_daily" >> $GITHUB_OUTPUT
            echo "skip_trading_check=true" >> $GITHUB_OUTPUT
            echo "skip_grok=false" >> $GITHUB_OUTPUT
            echo "üìä GROK Daily Mode (23:00 JST) - Grok selection enabled"
          else
            echo "mode=stock_data" >> $GITHUB_OUTPUT
            echo "skip_trading_check=false" >> $GITHUB_OUTPUT
            echo "skip_grok=true" >> $GITHUB_OUTPUT
            echo "üíπ Stock Data Mode (16:00 JST) - Grok selection skipped"
          fi

      - name: Check trading day window
        id: check_trading
        if: ${{ github.event.inputs.skip_trading_day_check != 'true' && steps.exec_mode.outputs.skip_trading_check != 'true' }}
        env:
          JQUANTS_REFRESH_TOKEN: ${{ secrets.JQUANTS_REFRESH_TOKEN }}
        run: |
          echo "üîç Checking if within trading day execution window..."
          python scripts/check_trading_day.py
          EXIT_CODE=$?

          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Within execution window (16:00-02:00 JST on trading day)"
          else
            echo "‚ùå Outside execution window or non-trading day"
          fi

          exit $EXIT_CODE

      - name: Skip if outside trading window
        if: |
          steps.check_updated.outputs.already_updated == 'true'
        run: |
          echo "::notice::Skipping pipeline execution - already updated today"
          exit 0

      - name: Run Data Pipeline
        id: pipeline
        env:
          JQUANTS_REFRESH_TOKEN: ${{ secrets.JQUANTS_REFRESH_TOKEN }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          FORCE_META_UPDATE: ${{ steps.check_trading.outputs.force_meta_update }}
          SKIP_GROK_GENERATION: ${{ steps.exec_mode.outputs.skip_grok }}
        run: |
          # Create .env.xai for Grok API
          echo "XAI_API_KEY=$XAI_API_KEY" > .env.xai

          echo "============================================================"
          echo "Data Pipeline Execution (GitHub Actions)"
          echo "Started at: $(TZ=Asia/Tokyo date +"%Y-%m-%d %H:%M:%S JST")"
          echo "============================================================"
          echo ""
          echo "Environment variables:"
          echo "  S3_BUCKET=${{ env.S3_BUCKET }}"
          echo "  PARQUET_PREFIX=${{ env.S3_PREFIX }}"
          echo "  AWS_REGION=${{ env.AWS_REGION }}"
          echo "  FORCE_META_UPDATE=$FORCE_META_UPDATE"
          echo "  SKIP_GROK_GENERATION=$SKIP_GROK_GENERATION"
          echo "  LATEST_TRADING_DAY=${{ steps.check_trading.outputs.latest_trading_day }}"
          echo ""

          # Force update flag for meta_jquants (manual trigger override)
          if [ "${{ github.event.inputs.force_meta_jquants }}" = "true" ]; then
            export FORCE_META_UPDATE=true
            echo "‚ö†Ô∏è Manual force update enabled for meta_jquants.parquet"
          fi

          # Run the NEW pipeline (scalping skip + Grok)
          # Old pipeline (with scalping): python scripts/run_pipeline.py
          python scripts/run_pipeline_scalping_skip_add_grok.py
          EXIT_CODE=$?

          echo ""
          echo "============================================================"
          echo "Pipeline completed with exit code: $EXIT_CODE"
          echo "============================================================"

          exit $EXIT_CODE

      - name: Run backtest and archive
        if: success()
        run: |
          echo "============================================================"
          echo "Running backtest and archiving results"
          echo "============================================================"

          # S3„Åã„ÇâÊó¢Â≠ò„ÅÆ„Ç¢„Éº„Ç´„Ç§„Éñ„Éï„Ç°„Ç§„É´„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ
          mkdir -p data/parquet/backtest
          ARCHIVE_FILE="grok_trending_archive.parquet"
          S3_ARCHIVE_PATH="s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$ARCHIVE_FILE"

          if aws s3 ls "$S3_ARCHIVE_PATH" > /dev/null 2>&1; then
            echo "üì• Downloading existing archive from S3..."
            aws s3 cp "$S3_ARCHIVE_PATH" "data/parquet/backtest/$ARCHIVE_FILE" --only-show-errors
            EXISTING_RECORDS=$(python -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/$ARCHIVE_FILE'); print(len(df))")
            EXISTING_DATES=$(python -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/$ARCHIVE_FILE'); print(df['backtest_date'].nunique())")
            echo "‚úÖ Downloaded archive: $EXISTING_RECORDS records across $EXISTING_DATES days"
          else
            echo "‚ÑπÔ∏è  No existing archive found in S3, will create new one"
          fi

          # „Éê„ÉÉ„ÇØ„ÉÜ„Çπ„Éà„ÇíÂÆüË°åÔºàÂΩìÊó•„ÅÆGROKÈÅ∏ÂÆöÁµêÊûú„Çí‰ΩøÁî®Ôºâ
          if [ -f "data/parquet/grok_trending.parquet" ]; then
            echo "‚úÖ Found grok_trending.parquet, running backtest..."
            python scripts/pipeline/save_backtest_to_archive.py

            if [ $? -eq 0 ]; then
              echo "‚úÖ Backtest completed and archived"
            else
              echo "‚ö†Ô∏è Backtest failed (non-critical)"
            fi
          else
            echo "‚ö†Ô∏è No grok_trending.parquet found, skipping backtest"
          fi

          echo "============================================================"

      - name: Extract data statistics
        id: stats
        if: success()
        run: |
          # meta_jquants statistics
          META_JQUANTS_COUNT=0
          if [ -f "data/parquet/meta_jquants.parquet" ]; then
            META_JQUANTS_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/meta_jquants.parquet')))")
          fi
          echo "meta_jquants_count=$META_JQUANTS_COUNT" >> $GITHUB_OUTPUT

          # grok_trending statistics (NEW)
          GROK_COUNT=0
          if [ -f "data/parquet/grok_trending.parquet" ]; then
            GROK_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/grok_trending.parquet')))")
          fi
          echo "grok_trending_count=$GROK_COUNT" >> $GITHUB_OUTPUT

          # grok_backtest_meta statistics (NEW)
          GROK_BACKTEST_EXISTS=false
          if [ -f "data/parquet/grok_backtest_meta.parquet" ]; then
            GROK_BACKTEST_EXISTS=true
          fi
          echo "grok_backtest_exists=$GROK_BACKTEST_EXISTS" >> $GITHUB_OUTPUT

          # grok_trending_archive statistics (NEW)
          ARCHIVE_TOTAL=0
          ARCHIVE_DATES=0
          if [ -f "data/parquet/backtest/grok_trending_archive.parquet" ]; then
            ARCHIVE_TOTAL=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/backtest/grok_trending_archive.parquet')))")
            ARCHIVE_DATES=$(python -c "import pandas as pd; df=pd.read_parquet('data/parquet/backtest/grok_trending_archive.parquet'); print(df['backtest_date'].nunique())")
          fi
          echo "archive_total=$ARCHIVE_TOTAL" >> $GITHUB_OUTPUT
          echo "archive_dates=$ARCHIVE_DATES" >> $GITHUB_OUTPUT

          # scalping statistics (kept for compatibility, will be 0)
          ENTRY_COUNT=0
          ACTIVE_COUNT=0
          if [ -f "data/parquet/scalping_entry.parquet" ]; then
            ENTRY_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/scalping_entry.parquet')))")
          fi
          if [ -f "data/parquet/scalping_active.parquet" ]; then
            ACTIVE_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/scalping_active.parquet')))")
          fi
          echo "scalping_entry_count=$ENTRY_COUNT" >> $GITHUB_OUTPUT
          echo "scalping_active_count=$ACTIVE_COUNT" >> $GITHUB_OUTPUT

          # all_stocks statistics
          ALL_STOCKS_COUNT=0
          if [ -f "data/parquet/all_stocks.parquet" ]; then
            ALL_STOCKS_COUNT=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/parquet/all_stocks.parquet')))")
          fi
          echo "all_stocks_count=$ALL_STOCKS_COUNT" >> $GITHUB_OUTPUT

          # prices statistics
          LATEST_DATE="N/A"
          if [ -f "data/parquet/prices_max_1d.parquet" ]; then
            LATEST_DATE=$(python -c "import pandas as pd; df=pd.read_parquet('data/parquet/prices_max_1d.parquet'); print(df['date'].max() if not df.empty else 'N/A')")
          fi
          echo "latest_price_date=$LATEST_DATE" >> $GITHUB_OUTPUT

          echo "üìä Statistics extracted:"
          echo "  - meta_jquants: $META_JQUANTS_COUNT stocks"
          echo "  - grok_trending: $GROK_COUNT stocks (NEW)"
          echo "  - grok_archive: $ARCHIVE_TOTAL records across $ARCHIVE_DATES days (NEW)"
          echo "  - scalping_entry: $ENTRY_COUNT stocks (skipped)"
          echo "  - scalping_active: $ACTIVE_COUNT stocks (skipped)"
          echo "  - all_stocks: $ALL_STOCKS_COUNT stocks"
          echo "  - latest_price_date: $LATEST_DATE"

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-output-${{ github.run_id }}
          path: |
            data/parquet/*.parquet
            data/parquet/manifest.json
          if-no-files-found: warn
          retention-days: 7

      - name: Send Slack notification (skipped 02:00 JST)
        if: |
          github.event.schedule == '0 17 * * *' &&
          steps.check_updated.outputs.already_updated == 'true'
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_INCOMING_WEBHOOK_URL }}
        run: |
          TODAY=$(TZ=Asia/Tokyo date +%Y-%m-%d)
          MESSAGE='{
            "text": "‚ÑπÔ∏è Data Pipeline Skipped (Already Updated)",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "‚ÑπÔ∏è *Data Pipeline Skipped*\nData already updated today (`'"${TODAY}"'`). Skipping 02:00 JST run."
                }
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*Branch:*\n`${{ github.ref_name }}`"},
                  {"type": "mrkdwn", "text": "*Trigger:*\n`02:00 JST scheduled run`"}
                ]
              },
              {
                "type": "actions",
                "elements": [
                  {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "View Workflow Run"},
                    "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }
            ]
          }'
          curl -X POST -H 'Content-type: application/json' --data "$MESSAGE" "$SLACK_WEBHOOK_URL" || true

      - name: Archive GROK trending for backtest
        id: archive-grok
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        run: |
          if [ -f "data/parquet/grok_trending.parquet" ]; then
            # cronÂà§ÂÆö
            if [ "${{ github.event.schedule }}" = "0 7 * * *" ]; then
              RUN_TIME="16:00"
            elif [ "${{ github.event.schedule }}" = "0 14 * * *" ]; then
              RUN_TIME="23:00"
            else
              RUN_TIME="manual"
            fi

            echo "üì¶ Archiving GROK trending for backtest ($RUN_TIME run)..."

            # Êó•‰ªòÂèñÂæóÔºàJSTÂü∫Ê∫ñ„ÄÅYYYYMMDDÂΩ¢ÂºèÔºâ
            DATE=$(TZ=Asia/Tokyo date +%Y%m%d)
            BACKTEST_FILE="grok_trending_${DATE}.parquet"
            BACKTEST_DIR="data/parquet/backtest"

            echo "[INFO] Run time: $RUN_TIME"
            echo "[INFO] JST Date: $DATE"
            echo "[INFO] Backup file: $BACKTEST_FILE (will overwrite if exists)"

            # „Éê„ÉÉ„ÇØ„ÉÜ„Çπ„Éà„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
            mkdir -p "$BACKTEST_DIR"

            # „Ç≥„Éî„Éº
            cp "data/parquet/grok_trending.parquet" "$BACKTEST_DIR/$BACKTEST_FILE"
            echo "‚úÖ Created: $BACKTEST_DIR/$BACKTEST_FILE"

            # S3„Å´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÔºà‰∏äÊõ∏„Åç‰øùÂ≠òÔºâ
            aws s3 cp "$BACKTEST_DIR/$BACKTEST_FILE" \
              "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$BACKTEST_FILE" \
              --region ${{ env.AWS_REGION }}
            echo "‚úÖ Uploaded to S3: s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$BACKTEST_FILE"

            # 7Êó•‰ª•Ââç„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§
            echo "üóëÔ∏è  Cleaning old backtest files (keeping last 7 days)..."
            CUTOFF_DATE=$(TZ=Asia/Tokyo date -d '7 days ago' +%Y%m%d)

            # S3‰∏ä„ÅÆ„Éê„ÉÉ„ÇØ„ÉÜ„Çπ„Éà„Éï„Ç°„Ç§„É´„Çí„É™„Çπ„Éà„Ç¢„ÉÉ„Éó
            aws s3 ls "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/" \
              --region ${{ env.AWS_REGION }} | while read -r line; do
              FILE=$(echo "$line" | awk '{print $4}')
              if [[ "$FILE" =~ grok_trending_([0-9]{8})\.parquet ]]; then
                FILE_DATE="${BASH_REMATCH[1]}"
                if [ "$FILE_DATE" -lt "$CUTOFF_DATE" ]; then
                  echo "  Deleting old file: $FILE (date: $FILE_DATE)"
                  aws s3 rm "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}backtest/$FILE" \
                    --region ${{ env.AWS_REGION }}
                fi
              fi
            done

            echo "‚úÖ Backtest archiving completed"
          else
            echo "‚ö†Ô∏è  grok_trending.parquet not found, skipping backtest archive"
          fi

      - name: Generate market summary
        if: success() && steps.exec_mode.outputs.skip_grok == 'true'
        env:
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
        run: |
          # cronÂà§ÂÆö
          if [ "${{ github.event.schedule }}" = "0 7 * * *" ]; then
            RUN_TIME="16:00"
          elif [ "${{ github.event.schedule }}" = "0 14 * * *" ]; then
            RUN_TIME="23:00"
          else
            RUN_TIME="manual"
          fi

          echo "üì∞ Generating market summary ($RUN_TIME run)..."

          # 16ÊôÇ„ÅÆ„ÅøÂ∏ÇÂ†¥„Çµ„Éû„É™„Éº„ÇíÁîüÊàêÔºàÂ§ßÂºï„ÅëÂæåÔºâ
          if [ "$RUN_TIME" = "16:00" ]; then
            python3 scripts/pipeline/generate_market_summary.py

            if [ $? -eq 0 ]; then
              echo "‚úÖ Market summary generated successfully"

              # ÁîüÊàê„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„ÇíÁ¢∫Ë™ç
              DATE=$(TZ=Asia/Tokyo date +%Y-%m-%d)
              RAW_FILE="data/parquet/market_summary/raw/${DATE}.md"
              JSON_FILE="data/parquet/market_summary/structured/${DATE}.json"

              if [ -f "$RAW_FILE" ] && [ -f "$JSON_FILE" ]; then
                echo "‚úÖ Files created:"
                echo "   - $RAW_FILE"
                echo "   - $JSON_FILE"

                # ÊñáÂ≠óÊï∞Á¢∫Ë™ç
                WORD_COUNT=$(wc -m < "$RAW_FILE")
                echo "   Word count: $WORD_COUNT characters"
              else
                echo "‚ö†Ô∏è  Warning: Expected files not found"
              fi
            else
              echo "‚ùå Market summary generation failed (non-critical, continuing)"
            fi
          else
            echo "‚ÑπÔ∏è  Skipping market summary (only runs at 16:00 JST)"
          fi

      - name: Extract GROK stocks info
        id: grok-info
        if: success()
        run: |
          # GROKÈäòÊüÑÊÉÖÂ†±„ÇíÊäΩÂá∫
          if [ -f "data/parquet/grok_trending.parquet" ]; then
            GROK_INFO=$(python3 -c '
          import pandas as pd
          import json

          df = pd.read_parquet("data/parquet/grok_trending.parquet")

          if df.empty:
              print("")
              exit(0)

          # selected_timeÂà•„ÅÆÈõÜË®à
          time_counts = df["selected_time"].value_counts().to_dict() if "selected_time" in df.columns else {}

          # ÈäòÊüÑ„É™„Çπ„ÉàÁîüÊàêÔºàÂÖ®ÈäòÊüÑÔºâ
          stocks_list = []
          for _, row in df.iterrows():
              ticker = row.get("ticker", "")
              stock_name = row.get("stock_name", "")
              reason = row.get("reason", "")
              tags = row.get("tags", "")
              selected_time = row.get("selected_time", "")

              # reason„Çí100ÊñáÂ≠ó„Å´Âàá„ÇäË©∞„ÇÅ
              reason_short = reason[:100] + "..." if len(reason) > 100 else reason

              stocks_list.append({
                  "ticker": ticker,
                  "stock_name": stock_name,
                  "reason": reason_short,
                  "tags": tags,
                  "selected_time": selected_time
              })

          result = {
              "total": len(df),
              "time_counts": time_counts,
              "stocks": stocks_list
          }

          print(json.dumps(result, ensure_ascii=False))
          ' 2>/dev/null)
          else
            GROK_INFO=""
          fi

          # „Éû„É´„ÉÅ„É©„Ç§„É≥ÂØæÂøú
          echo "grok_info<<EOF" >> $GITHUB_OUTPUT
          echo "$GROK_INFO" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Send Slack notification on success
        if: success() && steps.pipeline.conclusion == 'success'
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_INCOMING_WEBHOOK_URL }}
        run: |
          TRIGGER_TIME="16:00 JST"
          if [ "${{ github.event.schedule }}" = "0 14 * * *" ]; then
            TRIGGER_TIME="23:00 JST"
          elif [ "${{ github.event.schedule }}" = "0 17 * * *" ]; then
            TRIGGER_TIME="02:00 JST"
          fi

          # J-QuantsÈöúÂÆ≥Ê§úÁü•ÔºàÁ©∫„Éï„Ç°„Ç§„É´„Åã„Å©„ÅÜ„ÅãÔºâ
          JQUANTS_STATUS="‚úÖ Ê≠£Â∏∏"
          if [ "${{ steps.stats.outputs.meta_jquants_count }}" = "0" ]; then
            JQUANTS_STATUS="‚ö†Ô∏è ÈöúÂÆ≥Áô∫ÁîüÔºàÈùôÁöÑÈäòÊüÑ„ÅÆ„Åø„ÅßÊõ¥Êñ∞Ôºâ"
          fi

          # GROKÈäòÊüÑÊÉÖÂ†±„Çí„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºà23:00„ÅÆ„ÅøÔºâ
          GROK_SECTION=""
          MARKET_SUMMARY_SECTION=""

          if [ "${{ github.event.schedule }}" = "0 14 * * *" ]; then
            # 23:00 - GROKÈäòÊüÑ„ÇíË°®Á§∫
            GROK_INFO='${{ steps.grok-info.outputs.grok_info }}'

            if [ -n "$GROK_INFO" ] && [ "$GROK_INFO" != "null" ]; then
              # Âà•„Éï„Ç°„Ç§„É´„ÅÆPython„Çπ„ÇØ„É™„Éó„Éà„Åß„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºàYAML„Ç®„Çπ„Ç±„Éº„ÉóÂïèÈ°å„ÇíÂõûÈÅøÔºâ
              python3 scripts/format_grok_slack.py "$GROK_INFO"

              # „Éï„Ç°„Ç§„É´„Åã„ÇâË™≠„ÅøËæº„Åø
              if [ -f "/tmp/grok_section.txt" ] && [ -s "/tmp/grok_section.txt" ]; then
                GROK_SECTION=$(cat /tmp/grok_section.txt)
              fi
            fi
          elif [ "${{ github.event.schedule }}" = "0 7 * * *" ]; then
            # 16:00 - Â∏ÇÂ†¥„Çµ„Éû„É™„Éº„ÇíË°®Á§∫
            DATE=$(TZ=Asia/Tokyo date +%Y-%m-%d)
            JSON_FILE="data/parquet/market_summary/structured/${DATE}.json"

            if [ -f "$JSON_FILE" ]; then
              # JSON„Åã„Çâ‰∏ªË¶ÅÊåáÊï∞„Å®Ê≥®ÁõÆ„Éã„É•„Éº„Çπ„ÇíÊäΩÂá∫„Åó„Å¶SlackÁî®„Å´Êï¥ÂΩ¢
              SUMMARY_TEXT=$(python3 -c "
import json
import sys
import re

try:
    with open('${JSON_FILE}', 'r', encoding='utf-8') as f:
        data = json.load(f)

    content = data.get('content', {})
    metadata = data.get('report_metadata', {})

    # „Çø„Ç§„Éà„É´
    date = metadata.get('date', 'N/A')
    word_count = metadata.get('word_count', 0)

    # ÂÖ®‰Ωì„Éà„É¨„É≥„Éâ„ÇíÊäΩÂá∫ÔºàÁ∞°ÊΩî„Å´Ôºâ
    trends = content.get('sections', {}).get('trends', '')
    # ÊúÄÂàù„ÅÆÊÆµËêΩ„ÅÆ„ÅøÊäΩÂá∫ÔºàÁÆáÊù°Êõ∏„ÅçÈÉ®ÂàÜÔºâ
    trend_lines = []
    for line in trends.split('\n'):
        if line.startswith('- '):
            # URL„ÇíÂâäÈô§„Åó„Å¶Á∞°ÊΩî„Å´
            clean_line = re.sub(r'\[.*?\]\(.*?\)', '', line)
            clean_line = re.sub(r'http[s]?://\S+', '', clean_line)
            trend_lines.append(clean_line.strip())
        if len(trend_lines) >= 3:  # ÊúÄÂ§ß3Ë°å
            break

    trends_summary = '\n'.join(trend_lines) if trend_lines else 'Ôºà„Éá„Éº„ÇøÂèñÂæó‰∏≠Ôºâ'

    # Ê≥®ÁõÆ„Éã„É•„Éº„Çπ„ÅÆ„Çø„Ç§„Éà„É´„ÅÆ„ÅøÊäΩÂá∫
    news = content.get('sections', {}).get('news', '')
    news_titles = []
    for line in news.split('\n'):
        if line.startswith('- **') and '**:' in line:
            # „Çø„Ç§„Éà„É´ÈÉ®ÂàÜ„ÅÆ„ÅøÊäΩÂá∫
            title_match = re.search(r'\*\*(.*?)\*\*:', line)
            if title_match:
                news_titles.append(f'‚Ä¢ {title_match.group(1)}')
        if len(news_titles) >= 3:  # ÊúÄÂ§ß3„Å§
            break

    news_summary = '\n'.join(news_titles) if news_titles else 'Ôºà„Éá„Éº„ÇøÂèñÂæó‰∏≠Ôºâ'

    # SlackÁî®„ÉÜ„Ç≠„Çπ„Éà‰ΩúÊàêÔºàÁ∞°ÊΩîÁâàÔºâ
    summary = f'''*üìä ÂõΩÂÜÖÊ†™ÂºèÂ∏ÇÂ†¥„Çµ„Éû„É™„Éº* ({date})

*Â∏ÇÂ†¥ÂãïÂêë:*
{trends_summary}

*Ê≥®ÁõÆ„Éã„É•„Éº„Çπ:*
{news_summary}

_ÂÖ®{word_count}ÊñáÂ≠ó„ÅÆ„É¨„Éù„Éº„Éà ‚Üí \`GET /market-summary/latest\`_
'''
    print(summary)
except Exception as e:
    print(f'Error: {e}', file=sys.stderr)
    sys.exit(1)
" 2>/dev/null)

              if [ $? -eq 0 ] && [ -n "$SUMMARY_TEXT" ]; then
                # „Ç®„Çπ„Ç±„Éº„ÉóÂá¶ÁêÜ
                SUMMARY_TEXT_ESCAPED=$(echo "$SUMMARY_TEXT" | jq -Rs .)
                MARKET_SUMMARY_SECTION=',{
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": '"$SUMMARY_TEXT_ESCAPED"'
                  }
                }'
              fi
            fi
          fi

          MESSAGE='{
            "text": "‚úÖ Data Pipeline Succeeded",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "‚úÖ *Data Pipeline Succeeded*\nWorkflow `${{ github.workflow }}` completed successfully."
                }
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*Branch:*\n`${{ github.ref_name }}`"},
                  {"type": "mrkdwn", "text": "*Trigger:*\n`'"${TRIGGER_TIME}"'`"},
                  {"type": "mrkdwn", "text": "*J-Quants:*\n'"${JQUANTS_STATUS}"'"},
                  {"type": "mrkdwn", "text": "*Latest Price Date:*\n`${{ steps.stats.outputs.latest_price_date }}`"}
                ]
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*meta_jquants:*\n`${{ steps.stats.outputs.meta_jquants_count }}`ÈäòÊüÑ"},
                  {"type": "mrkdwn", "text": "*all_stocks:*\n`${{ steps.stats.outputs.all_stocks_count }}`ÈäòÊüÑ"},
                  {"type": "mrkdwn", "text": "*grok_trending:*\n`${{ steps.stats.outputs.grok_trending_count }}`ÈäòÊüÑ (NEW)"},
                  {"type": "mrkdwn", "text": "*scalping (skipped):*\n`${{ steps.stats.outputs.scalping_entry_count }}`/`${{ steps.stats.outputs.scalping_active_count }}`ÈäòÊüÑ"}
                ]
              },
              {
                "type": "actions",
                "elements": [
                  {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "View Workflow Run"},
                    "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }'"$GROK_SECTION$MARKET_SUMMARY_SECTION"'
            ]
          }'
          curl -X POST -H 'Content-type: application/json' --data "$MESSAGE" "$SLACK_WEBHOOK_URL" || true

      - name: Send Slack notification on failure
        if: failure()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_INCOMING_WEBHOOK_URL }}
        run: |
          TRIGGER_TIME="16:00 JST"
          if [ "${{ github.event.schedule }}" = "0 17 * * *" ]; then
            TRIGGER_TIME="02:00 JST"
          fi

          MESSAGE='{
            "text": "‚ùå Data Pipeline Failed",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "‚ùå *Data Pipeline Failed*\nWorkflow `${{ github.workflow }}` encountered an error."
                }
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*Branch:*\n`${{ github.ref_name }}`"},
                  {"type": "mrkdwn", "text": "*Trigger:*\n`'"${TRIGGER_TIME}"'`"},
                  {"type": "mrkdwn", "text": "*Event:*\n`${{ github.event_name }}`"}
                ]
              },
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "‚ö†Ô∏è *Action Required*\nPlease check the workflow logs and investigate the failure."
                }
              },
              {
                "type": "actions",
                "elements": [
                  {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "View Workflow Run"},
                    "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }
            ]
          }'
          curl -X POST -H 'Content-type: application/json' --data "$MESSAGE" "$SLACK_WEBHOOK_URL" || true

  deploy:
    name: Trigger ECR Deployment
    needs: pipeline
    if: success()
    uses: ./.github/workflows/deploy-ecr.yml
    secrets: inherit
    permissions:
      id-token: write
      contents: read
