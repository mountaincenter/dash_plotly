#!/usr/bin/env python3
"""
cleanup_grok_trending.py

S3ä¸Šã®grok_trending.parquet ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
ã‚«ãƒ©ãƒ æ§‹é€ ã¯ç¶­æŒã—ãŸã¾ã¾ã€å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤

âš ï¸ é‡è¦: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å¿…ãšãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèªå¾Œã«å®Ÿè¡Œã™ã‚‹ã“ã¨
"""

import sys
import argparse
from pathlib import Path

import pandas as pd

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from common_cfg.s3io import upload_file, download_file
from common_cfg.s3cfg import load_s3_config


def cleanup_grok_trending(s3_key: str = "grok_trending.parquet", dry_run: bool = False) -> bool:
    """
    S3ä¸Šã®grok_trending.parquet ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

    Args:
        s3_key: S3ã‚­ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: grok_trending.parquetï¼‰
        dry_run: True ã®å ´åˆã¯å®Ÿéš›ã«ã¯æ›¸ãè¾¼ã¾ãªã„

    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆ True
    """
    # S3è¨­å®šã‚’èª­ã¿è¾¼ã¿
    cfg = load_s3_config()
    if not cfg:
        print("âŒ S3 not configured")
        return False

    print("=" * 60)
    print("Cleanup grok_trending.parquet on S3")
    print("=" * 60)
    print(f"S3 Key: s3://{cfg.bucket}/{s3_key}")

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    temp_dir = Path("data/parquet/temp")
    temp_dir.mkdir(parents=True, exist_ok=True)
    temp_file = temp_dir / "grok_trending_temp.parquet"

    # S3ã‹ã‚‰ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    try:
        print(f"\nğŸ“¥ Downloading from S3...")
        if not download_file(cfg, s3_key, temp_file):
            print(f"âŒ Failed to download from S3: {s3_key}")
            return False

        df_current = pd.read_parquet(temp_file)
        print(f"\nCurrent data:")
        print(f"  Rows: {len(df_current)}")
        print(f"  Columns: {len(df_current.columns)}")

        if not df_current.empty:
            print(f"  Date: {df_current['date'].iloc[0] if 'date' in df_current.columns else 'N/A'}")

            if 'code' in df_current.columns:
                codes = df_current['code'].tolist()
                print(f"  Codes ({len(codes)}): {', '.join(str(c) for c in codes[:10])}")
                if len(codes) > 10:
                    print(f"           ... and {len(codes) - 10} more")

    except Exception as e:
        print(f"âŒ Error reading current file: {e}")
        return False

    # ã‚«ãƒ©ãƒ æ§‹é€ ã‚’ç¶­æŒã—ãŸã¾ã¾ç©ºã®DataFrameã‚’ä½œæˆ
    try:
        # å…ƒã®ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—
        schema = df_current.dtypes.to_dict()

        # ç©ºã®DataFrameã‚’ä½œæˆ
        df_empty = pd.DataFrame(columns=df_current.columns)

        # å‹ã‚’å¾©å…ƒ
        for col, dtype in schema.items():
            df_empty[col] = df_empty[col].astype(dtype)

        print(f"\nNew data (empty):")
        print(f"  Rows: {len(df_empty)}")
        print(f"  Columns: {len(df_empty.columns)}")
        print(f"  Column names: {df_empty.columns.tolist()[:5]}...")
        print(f"  Dtypes preserved: {all(df_empty.dtypes == df_current.dtypes)}")

    except Exception as e:
        print(f"âŒ Error creating empty DataFrame: {e}")
        return False

    # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if dry_run:
        print("\nâš ï¸ DRY RUN mode - not uploading to S3")
        print("âœ… Cleanup would succeed")
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        temp_file.unlink(missing_ok=True)
        return True

    try:
        # ç©ºã®DataFrameã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        df_empty.to_parquet(temp_file, index=False, engine='pyarrow')
        print(f"\nâœ… Created empty file locally: {temp_file}")

        # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print(f"ğŸ“¤ Uploading to S3...")
        upload_file(cfg, temp_file, s3_key)
        print(f"âœ… Successfully uploaded to S3: s3://{cfg.bucket}/{s3_key}")

        # ç¢ºèªã®ãŸã‚èª­ã¿è¾¼ã¿
        df_verify = pd.read_parquet(temp_file)
        print(f"\nVerification:")
        print(f"  Rows: {len(df_verify)}")
        print(f"  Columns: {len(df_verify.columns)}")

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        temp_file.unlink(missing_ok=True)

        if len(df_verify) == 0 and len(df_verify.columns) == len(df_current.columns):
            print("âœ… Cleanup verified successfully")
            return True
        else:
            print("âš ï¸ Unexpected verification result")
            return False

    except Exception as e:
        print(f"âŒ Error uploading cleaned file: {e}")
        import traceback
        traceback.print_exc()
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        temp_file.unlink(missing_ok=True)
        return False


def main() -> int:
    parser = argparse.ArgumentParser(
        description="S3ä¸Šã®grok_trending.parquet ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆã‚«ãƒ©ãƒ æ§‹é€ ç¶­æŒã€å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤ï¼‰"
    )
    parser.add_argument(
        '--s3-key',
        default='grok_trending.parquet',
        help='S3ã‚­ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: grok_trending.parquetï¼‰'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='DRY RUN ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã«ã¯S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãªã„ï¼‰'
    )

    args = parser.parse_args()

    print("\nâš ï¸ IMPORTANT: This script should only run AFTER backup verification")
    print("   Make sure verify_grok_backup.py has completed successfully\n")

    success = cleanup_grok_trending(args.s3_key, args.dry_run)

    if success:
        print("\n" + "=" * 60)
        print("âœ… Cleanup completed successfully")
        print("=" * 60)
        return 0
    else:
        print("\n" + "=" * 60)
        print("âŒ Cleanup failed")
        print("=" * 60)
        return 1


if __name__ == '__main__':
    sys.exit(main())
