#!/usr/bin/env python3
"""
grok_day_trade_list.parquet ã« grok_trending.parquet ã®éŠ˜æŸ„ã‚’è¿½åŠ ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. S3ã‹ã‚‰ grok_day_trade_list.parquet ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
2. grok_trending.parquet ã‹ã‚‰æ–°è¦éŠ˜æŸ„ã‚’æŠ½å‡º
3. æ—¢å­˜éŠ˜æŸ„ã¯ä¸Šæ›¸ãã›ãšã€æ–°è¦éŠ˜æŸ„ã®ã¿è¿½åŠ 
4. S3ã« grok_day_trade_list.parquet ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

é‡è¦: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ä¸Šæ›¸ãã¯å³ç¦
"""

import os
import sys
import re
import pandas as pd
import boto3
from io import BytesIO

# è¨­å®š
S3_BUCKET = os.environ.get("S3_BUCKET", "stock-api-data")
S3_PREFIX = os.environ.get("S3_PREFIX", "parquet/")
DAY_TRADE_LIST_FILE = "grok_day_trade_list.parquet"
META_JQUANTS_FILE = "meta_jquants.parquet"
GROK_TRENDING_PATH = "data/parquet/grok_trending.parquet"

# tickerå½¢å¼ã®æ¤œè¨¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆXXXX.Tï¼‰
TICKER_PATTERN = re.compile(r'^\d+[A-Z]?\.T$')

# å¿…é ˆã‚«ãƒ©ãƒ 
EXPECTED_COLUMNS = ['ticker', 'stock_name', 'shortable', 'day_trade', 'ng', 'day_trade_available_shares']


def download_from_s3(s3_client, key: str) -> pd.DataFrame | None:
    """S3ã‹ã‚‰parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        response = s3_client.get_object(Bucket=S3_BUCKET, Key=key)
        return pd.read_parquet(BytesIO(response["Body"].read()))
    except s3_client.exceptions.NoSuchKey:
        print(f"  âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: s3://{S3_BUCKET}/{key}")
        return None
    except Exception as e:
        print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def upload_to_s3(s3_client, df: pd.DataFrame, key: str) -> bool:
    """DataFrameã‚’S3ã«parquetå½¢å¼ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    try:
        buffer = BytesIO()
        df.to_parquet(buffer, index=False)
        buffer.seek(0)
        s3_client.put_object(Bucket=S3_BUCKET, Key=key, Body=buffer.getvalue())
        return True
    except Exception as e:
        print(f"  âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def validate_dataframe(df: pd.DataFrame) -> tuple[bool, list[str]]:
    """DataFrameã®æ¤œè¨¼: tickerå½¢å¼ã¨ã‚«ãƒ©ãƒ ã‚’ãƒã‚§ãƒƒã‚¯"""
    errors = []

    # ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
    missing_cols = set(EXPECTED_COLUMNS) - set(df.columns)
    if missing_cols:
        errors.append(f"ã‚«ãƒ©ãƒ ä¸è¶³: {missing_cols}")

    # tickerå½¢å¼ãƒã‚§ãƒƒã‚¯
    invalid_tickers = df[~df['ticker'].apply(lambda x: bool(TICKER_PATTERN.match(str(x))))]['ticker'].tolist()
    if invalid_tickers:
        errors.append(f"ä¸æ­£ãªtickerå½¢å¼: {invalid_tickers[:5]}...")  # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º

    return len(errors) == 0, errors


def main():
    print("=" * 60)
    print("grok_day_trade_list.parquet éŠ˜æŸ„è¿½åŠ å‡¦ç†")
    print("=" * 60)
    print()

    # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    s3_client = boto3.client("s3")
    s3_key = f"{S3_PREFIX}{DAY_TRADE_LIST_FILE}"

    # Step 1: grok_trending.parquet ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
    print("ğŸ“¥ Step 1: grok_trending.parquet ã‚’èª­ã¿è¾¼ã¿...")
    if not os.path.exists(GROK_TRENDING_PATH):
        print(f"  âŒ ERROR: {GROK_TRENDING_PATH} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    df_grok = pd.read_parquet(GROK_TRENDING_PATH)
    grok_tickers = set(df_grok["ticker"].unique())
    print(f"  âœ… grok_trending: {len(grok_tickers)} éŠ˜æŸ„")

    # Step 2: S3ã‹ã‚‰ grok_day_trade_list.parquet ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    print()
    print(f"ğŸ“¥ Step 2: S3ã‹ã‚‰ {DAY_TRADE_LIST_FILE} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰...")
    df_master = download_from_s3(s3_client, s3_key)

    if df_master is None:
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã¯å‡¦ç†ã‚’ä¸­æ–­ï¼ˆä¸Šæ›¸ãé˜²æ­¢ï¼‰
        print("  âŒ ERROR: S3ã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“")
        print("  âš ï¸ ä¸Šæ›¸ãé˜²æ­¢ã®ãŸã‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™")
        sys.exit(1)
    else:
        # æ¤œè¨¼
        is_valid, errors = validate_dataframe(df_master)
        if not is_valid:
            print(f"  âŒ ERROR: S3ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã¾ã™")
            for err in errors:
                print(f"    - {err}")
            print("  âš ï¸ å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚æ‰‹å‹•ã§ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)
        existing_tickers = set(df_master["ticker"].unique())
        print(f"  âœ… æ—¢å­˜: {len(existing_tickers)} éŠ˜æŸ„ï¼ˆæ¤œè¨¼OKï¼‰")

    # Step 2.5: meta_jquants.parquet ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆéŠ˜æŸ„åå–å¾—ç”¨ï¼‰
    print()
    print(f"ğŸ“¥ Step 2.5: S3ã‹ã‚‰ {META_JQUANTS_FILE} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰...")
    meta_key = f"{S3_PREFIX}{META_JQUANTS_FILE}"
    df_meta = download_from_s3(s3_client, meta_key)
    if df_meta is None:
        print("  âŒ ERROR: meta_jquants.parquet ãŒå–å¾—ã§ãã¾ã›ã‚“")
        sys.exit(1)
    print(f"  âœ… meta_jquants: {len(df_meta)} éŠ˜æŸ„")

    # Step 3: æ–°è¦éŠ˜æŸ„ã®ã¿æŠ½å‡ºï¼ˆä¸Šæ›¸ãå³ç¦ï¼‰
    print()
    print("ğŸ” Step 3: æ–°è¦éŠ˜æŸ„ã‚’æŠ½å‡º...")
    new_tickers = grok_tickers - existing_tickers

    if not new_tickers:
        print("  â„¹ï¸ æ–°è¦éŠ˜æŸ„ãªã— - å‡¦ç†çµ‚äº†")
        print()
        print("âœ… å®Œäº†ï¼ˆå¤‰æ›´ãªã—ï¼‰")
        return

    print(f"  ğŸ“Š æ–°è¦éŠ˜æŸ„: {len(new_tickers)} ä»¶")
    for ticker in sorted(new_tickers):
        print(f"    - {ticker}")

    # Step 4: æ–°è¦éŠ˜æŸ„ã‚’è¿½åŠ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ + éŠ˜æŸ„åå–å¾—ï¼‰
    print()
    print("â• Step 4: æ–°è¦éŠ˜æŸ„ã‚’è¿½åŠ ...")

    # meta_jquantsã‹ã‚‰éŠ˜æŸ„åã‚’å–å¾—
    meta_dict = df_meta.set_index('ticker')['stock_name'].to_dict()

    new_rows = pd.DataFrame(
        {
            "ticker": list(new_tickers),
            "stock_name": [meta_dict.get(t, None) for t in new_tickers],
            "shortable": False,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç©ºå£²ã‚Šä¸å¯
            "day_trade": True,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã„ã¡ã«ã¡ä¿¡ç”¨å¯
            "ng": False,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å–å¼•å¯
            "day_trade_available_shares": None,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœªè¨­å®š
        }
    )

    # éŠ˜æŸ„åãŒå–å¾—ã§ããªã‹ã£ãŸéŠ˜æŸ„ã‚’è­¦å‘Š
    missing_names = new_rows[new_rows['stock_name'].isna()]['ticker'].tolist()
    if missing_names:
        print(f"  âš ï¸ éŠ˜æŸ„åãŒå–å¾—ã§ããªã„éŠ˜æŸ„: {missing_names}")

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¤ã¤æ–°è¦è¿½åŠ 
    df_updated = pd.concat([df_master, new_rows], ignore_index=True)

    # ã‚«ãƒ©ãƒ é †åºã‚’çµ±ä¸€
    df_updated = df_updated[EXPECTED_COLUMNS]

    # ticker ã§ã‚½ãƒ¼ãƒˆ
    df_updated = df_updated.sort_values("ticker").reset_index(drop=True)

    print(f"  âœ… æ›´æ–°å¾Œ: {len(df_updated)} éŠ˜æŸ„ (+{len(new_tickers)})")

    # æœ€çµ‚æ¤œè¨¼
    is_valid, errors = validate_dataframe(df_updated)
    if not is_valid:
        print(f"  âŒ ERROR: æ›´æ–°å¾Œãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã«å¤±æ•—")
        for err in errors:
            print(f"    - {err}")
        sys.exit(1)

    # Step 5: S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    print()
    print(f"ğŸ“¤ Step 5: S3ã« {DAY_TRADE_LIST_FILE} ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰...")
    if upload_to_s3(s3_client, df_updated, s3_key):
        print(f"  âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{S3_BUCKET}/{s3_key}")
    else:
        print("  âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
        sys.exit(1)

    print()
    print("=" * 60)
    print(f"âœ… å®Œäº†: {len(new_tickers)} éŠ˜æŸ„ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    print("=" * 60)


if __name__ == "__main__":
    main()
