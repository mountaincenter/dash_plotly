#!/usr/bin/env python3
"""
ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ¤œè¨¼

tech_snapshot_history.parquet ã‚’ä½¿ç”¨ã—ã¦ï¼š
1. å„æ—¥ã®å„éŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—
2. ç¿Œæ—¥ãƒ»ç¿Œé€±ã®ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—
3. ã‚¹ã‚³ã‚¢ã¨ãƒªã‚¿ãƒ¼ãƒ³ã®ç›¸é–¢ã‚’åˆ†æ
4. ã‚·ã‚°ãƒŠãƒ«åˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡

å‡ºåŠ›: test_output/backtest_validation_YYYYMMDD.json
"""

import sys
from pathlib import Path
from datetime import datetime
import json

ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import pandas as pd
import numpy as np
from tqdm import tqdm

PARQUET_DIR = ROOT / 'data' / 'parquet'
TEST_OUTPUT_DIR = ROOT / 'test_output'


def load_data():
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    print("[1/5] Loading data...")

    # å±¥æ­´ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿
    tech_history = pd.read_parquet(PARQUET_DIR / 'tech_snapshot_history.parquet')

    # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
    prices = pd.read_parquet(PARQUET_DIR / 'prices_max_1d.parquet')
    prices['date'] = pd.to_datetime(prices['date'])

    print(f"  âœ“ tech_history: {len(tech_history):,} records")
    print(f"  âœ“ prices: {len(prices):,} records")

    return tech_history, prices


def calculate_future_returns(prices):
    """å„æ—¥ã®ç¿Œæ—¥ãƒ»ç¿Œé€±ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—"""
    print("\n[2/5] Calculating future returns...")

    returns_data = []

    for ticker in tqdm(prices['ticker'].unique(), desc="Processing tickers"):
        ticker_data = prices[prices['ticker'] == ticker].sort_values('date').copy()

        # ç¿Œæ—¥ãƒªã‚¿ãƒ¼ãƒ³
        ticker_data['next_day_return'] = ticker_data['Close'].pct_change().shift(-1) * 100

        # ç¿Œé€±ãƒªã‚¿ãƒ¼ãƒ³ (5å–¶æ¥­æ—¥å¾Œ)
        ticker_data['next_week_return'] = (ticker_data['Close'].shift(-5) / ticker_data['Close'] - 1) * 100

        returns_data.append(ticker_data[['ticker', 'date', 'Close', 'next_day_return', 'next_week_return']])

    returns_df = pd.concat(returns_data, ignore_index=True)
    print(f"  âœ“ Calculated returns for {len(returns_df):,} records")

    return returns_df


def merge_tech_and_returns(tech_history, returns_df):
    """ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚¹ã‚³ã‚¢ã¨ãƒªã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒ¼ã‚¸"""
    print("\n[3/5] Merging technical scores with returns...")

    # tech_history ã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    tech_scores = []

    for idx, row in tech_history.iterrows():
        overall = row['overall']
        tech_scores.append({
            'ticker': row['ticker'],
            'date': pd.to_datetime(row['date']),
            'tech_label': overall['label'],
            'tech_score': overall['score']
        })

    tech_df = pd.DataFrame(tech_scores)

    # ãƒãƒ¼ã‚¸
    merged = pd.merge(
        tech_df,
        returns_df,
        on=['ticker', 'date'],
        how='inner'
    )

    # NaNã‚’é™¤å¤–
    merged = merged.dropna(subset=['next_day_return', 'next_week_return'])

    print(f"  âœ“ Merged: {len(merged):,} records")

    return merged


def analyze_performance(merged):
    """ã‚·ã‚°ãƒŠãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æ"""
    print("\n[4/5] Analyzing signal performance...")

    results = {
        'overall_stats': {},
        'signal_performance': {},
        'score_correlation': {}
    }

    # å…¨ä½“çµ±è¨ˆ
    results['overall_stats'] = {
        'total_records': len(merged),
        'avg_next_day_return': float(merged['next_day_return'].mean()),
        'avg_next_week_return': float(merged['next_week_return'].mean()),
        'score_range': {
            'min': int(merged['tech_score'].min()),
            'max': int(merged['tech_score'].max()),
            'mean': float(merged['tech_score'].mean())
        }
    }

    # ã‚·ã‚°ãƒŠãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    for label in ['å¼·ã„è²·ã„', 'è²·ã„', 'ä¸­ç«‹', 'å£²ã‚Š', 'å¼·ã„å£²ã‚Š']:
        signal_data = merged[merged['tech_label'] == label]

        if len(signal_data) == 0:
            continue

        results['signal_performance'][label] = {
            'count': len(signal_data),
            'avg_next_day_return': float(signal_data['next_day_return'].mean()),
            'avg_next_week_return': float(signal_data['next_week_return'].mean()),
            'next_day_win_rate': float((signal_data['next_day_return'] > 0).mean() * 100),
            'next_week_win_rate': float((signal_data['next_week_return'] > 0).mean() * 100),
            'next_day_median': float(signal_data['next_day_return'].median()),
            'next_week_median': float(signal_data['next_week_return'].median())
        }

    # ã‚¹ã‚³ã‚¢ã¨ãƒªã‚¿ãƒ¼ãƒ³ã®ç›¸é–¢
    results['score_correlation'] = {
        'next_day': float(merged['tech_score'].corr(merged['next_day_return'])),
        'next_week': float(merged['tech_score'].corr(merged['next_week_return']))
    }

    # ã‚¹ã‚³ã‚¢ç¯„å›²åˆ¥ã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³
    merged['score_bin'] = pd.cut(
        merged['tech_score'],
        bins=[-20, -10, -5, 0, 5, 10, 20],
        labels=['<-10', '-10~-5', '-5~0', '0~5', '5~10', '>10']
    )

    score_bins = {}
    for bin_label in merged['score_bin'].unique():
        if pd.isna(bin_label):
            continue
        bin_data = merged[merged['score_bin'] == bin_label]
        score_bins[str(bin_label)] = {
            'count': len(bin_data),
            'avg_next_day_return': float(bin_data['next_day_return'].mean()),
            'avg_next_week_return': float(bin_data['next_week_return'].mean())
        }

    results['score_bins'] = score_bins

    # ã‚µãƒãƒªãƒ¼å‡ºåŠ›
    print("\n  ğŸ“Š Signal Performance Summary:")
    for label, perf in results['signal_performance'].items():
        print(f"    {label}:")
        print(f"      Count: {perf['count']}")
        print(f"      Next Day: {perf['avg_next_day_return']:+.2f}% (å‹ç‡ {perf['next_day_win_rate']:.1f}%)")
        print(f"      Next Week: {perf['avg_next_week_return']:+.2f}% (å‹ç‡ {perf['next_week_win_rate']:.1f}%)")

    print(f"\n  ğŸ“ˆ Score Correlation:")
    print(f"    Next Day: {results['score_correlation']['next_day']:.4f}")
    print(f"    Next Week: {results['score_correlation']['next_week']:.4f}")

    return results


def save_results(results):
    """çµæœã‚’ä¿å­˜"""
    print("\n[5/5] Saving results...")

    today = datetime.now().strftime('%Y%m%d')
    output_file = TEST_OUTPUT_DIR / f'backtest_validation_{today}.json'

    output = {
        'generated_at': datetime.now().isoformat(),
        'date': today,
        **results
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"  âœ“ Saved: {output_file}")

    return output_file


def main():
    print("=" * 60)
    print("Backtest: Technical Scoring Validation")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    tech_history, prices = load_data()

    # å°†æ¥ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
    returns_df = calculate_future_returns(prices)

    # ãƒãƒ¼ã‚¸
    merged = merge_tech_and_returns(tech_history, returns_df)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    results = analyze_performance(merged)

    # ä¿å­˜
    output_file = save_results(results)

    print("\nâœ… Backtest validation completed!")
    print(f"\nNext: Generate HTML report with backtest results")

    return 0


if __name__ == '__main__':
    sys.exit(main())
