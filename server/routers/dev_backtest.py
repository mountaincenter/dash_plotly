# server/routers/dev_backtest.py
"""
é–‹ç™ºè€…å‘ã‘GROKãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœAPI
/api/dev/backtest/* - JSON APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
"""
from fastapi import APIRouter, HTTPException
from pathlib import Path
import pandas as pd
from datetime import datetime, date
from typing import List, Dict, Any
import sys
import os
import boto3
from botocore.exceptions import ClientError

ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from common_cfg.paths import PARQUET_DIR

router = APIRouter()

BACKTEST_DIR = PARQUET_DIR / "backtest"
ARCHIVE_FILE = BACKTEST_DIR / "grok_trending_archive.parquet"

# S3è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
S3_BUCKET = os.getenv("S3_BUCKET", "stock-api-data")
S3_PREFIX = os.getenv("S3_PREFIX", "parquet/")
AWS_REGION = os.getenv("AWS_REGION", "ap-northeast-1")


def load_archive_data() -> pd.DataFrame:
    """
    ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    - ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
    - ãªã‘ã‚Œã°S3ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
    """
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
    if ARCHIVE_FILE.exists():
        df = pd.read_parquet(ARCHIVE_FILE)
        if 'backtest_date' in df.columns:
            df['backtest_date'] = pd.to_datetime(df['backtest_date'])
        return df

    # S3ã‹ã‚‰èª­ã¿è¾¼ã¿
    try:
        s3_key = f"{S3_PREFIX}backtest/grok_trending_archive.parquet"
        s3_url = f"s3://{S3_BUCKET}/{s3_key}"

        print(f"[INFO] Loading backtest archive from S3: {s3_url}")

        # S3ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ï¼ˆpandas.read_parquet ã¯s3://ã‚’ã‚µãƒãƒ¼ãƒˆï¼‰
        df = pd.read_parquet(s3_url, storage_options={
            "client_kwargs": {"region_name": AWS_REGION}
        })

        if 'backtest_date' in df.columns:
            df['backtest_date'] = pd.to_datetime(df['backtest_date'])

        print(f"[INFO] Successfully loaded {len(df)} records from S3")
        return df

    except Exception as e:
        print(f"[WARNING] Could not load backtest archive from S3: {type(e).__name__}: {e}")
        return pd.DataFrame()


def calculate_daily_stats(df: pd.DataFrame) -> Dict[str, Any]:
    """æ—¥åˆ¥çµ±è¨ˆã‚’è¨ˆç®—"""
    # phase1_returnã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if 'phase1_return' not in df.columns:
        return {
            "total_stocks": len(df),
            "valid_results": 0,
            "avg_return": None,
            "win_rate": None,
            "max_return": None,
            "min_return": None,
            "top5_avg_return": None,
            "top5_win_rate": None,
        }

    valid_results = df['phase1_return'].notna()
    df_valid = df[valid_results]

    if len(df_valid) == 0:
        return {
            "total_stocks": len(df),
            "valid_results": 0,
            "avg_return": None,
            "win_rate": None,
            "max_return": None,
            "min_return": None,
            "top5_avg_return": None,
            "top5_win_rate": None,
        }

    # å…¨ä½“çµ±è¨ˆ
    avg_return = float(df_valid['phase1_return'].mean())
    win_count = (df_valid['phase1_win'] == True).sum()
    win_rate = float(win_count / len(df_valid) * 100)
    max_return = float(df_valid['phase1_return'].max())
    min_return = float(df_valid['phase1_return'].min())

    # ç´¯è¨ˆæç›Šï¼ˆ100æ ªã‚ãŸã‚Šï¼‰
    total_profit_per_100 = 0.0
    if 'buy_price' in df_valid.columns and 'sell_price' in df_valid.columns:
        profits = (df_valid['sell_price'] - df_valid['buy_price']) * 100
        total_profit_per_100 = float(profits.sum())

    # Top5çµ±è¨ˆ
    if 'grok_rank' in df.columns:
        df_top5 = df[df['grok_rank'] <= 5]
        df_top5_valid = df_top5[df_top5['phase1_return'].notna()]
    else:
        df_top5_valid = pd.DataFrame()

    top5_avg_return = None
    top5_win_rate = None
    top5_total_profit_per_100 = None

    if len(df_top5_valid) > 0:
        top5_avg_return = float(df_top5_valid['phase1_return'].mean())
        top5_win_count = (df_top5_valid['phase1_win'] == True).sum()
        top5_win_rate = float(top5_win_count / len(df_top5_valid) * 100)

        # Top5ç´¯è¨ˆæç›Š
        if 'buy_price' in df_top5_valid.columns and 'sell_price' in df_top5_valid.columns:
            top5_profits = (df_top5_valid['sell_price'] - df_top5_valid['buy_price']) * 100
            top5_total_profit_per_100 = float(top5_profits.sum())

    return {
        "total_stocks": len(df),
        "valid_results": len(df_valid),
        "avg_return": avg_return,
        "win_rate": win_rate,
        "max_return": max_return,
        "min_return": min_return,
        "total_profit_per_100": total_profit_per_100,
        "top5_avg_return": top5_avg_return,
        "top5_win_rate": top5_win_rate,
        "top5_total_profit_per_100": top5_total_profit_per_100,
    }


@router.get("/api/dev/backtest/summary")
async def get_backtest_summary(
    prompt_version: str | None = None,
    phase: str = "phase1"
):
    """
    ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå…¨ä½“ã‚µãƒãƒªãƒ¼ï¼ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ï¼‰

    Args:
        prompt_version: ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ (ä¾‹: "v1_0_baseline")
                       æŒ‡å®šã—ãªã„å ´åˆã¯å…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        phase: è¡¨ç¤ºã™ã‚‹Phase (phase1, phase2, phase3)
               - phase1: å‰å ´å¼•ã‘å£²ã‚Šï¼ˆ11:30å£²å´ï¼‰
               - phase2: å¤§å¼•ã‘å£²ã‚Šï¼ˆ15:30å£²å´ï¼‰
               - phase3: +3%åˆ©ç¢º/-3%æåˆ‡ã‚Š
    """
    df_all = load_archive_data()

    if df_all.empty:
        raise HTTPException(status_code=404, detail="No backtest data found")

    # Phaseã«å¿œã˜ãŸã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°
    phase_config = {
        "phase1": {
            "return_col": "phase1_return",
            "win_col": "phase1_win",
            "profit_col": "profit_per_100_shares_phase1",
            "description": "å‰å ´å¼•ã‘å£²ã‚Šï¼ˆ11:30å£²å´ï¼‰"
        },
        "phase2": {
            "return_col": "phase2_return",
            "win_col": "phase2_win",
            "profit_col": "profit_per_100_shares_phase2",
            "description": "å¤§å¼•ã‘å£²ã‚Šï¼ˆ15:30å£²å´ï¼‰"
        },
        "phase3": {
            "return_col": "phase3_3pct_return",
            "win_col": "phase3_3pct_win",
            "profit_col": "profit_per_100_shares_phase3_3pct",
            "description": "+3%åˆ©ç¢º/-3%æåˆ‡ã‚Š"
        }
    }

    # Phaseæ¤œè¨¼
    if phase not in phase_config:
        raise HTTPException(status_code=400, detail=f"Invalid phase: {phase}. Must be one of: {list(phase_config.keys())}")

    config = phase_config[phase]
    return_col = config["return_col"]
    win_col = config["win_col"]
    profit_col = config["profit_col"]

    # åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—
    available_versions = []
    if 'prompt_version' in df_all.columns:
        available_versions = sorted(df_all['prompt_version'].unique().tolist())

    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
    current_version = prompt_version if prompt_version else "all"
    if prompt_version and 'prompt_version' in df_all.columns:
        df_all = df_all[df_all['prompt_version'] == prompt_version].copy()
        if df_all.empty:
            raise HTTPException(
                status_code=404,
                detail=f"No data found for version: {prompt_version}"
            )

    # å…¨ä½“ã®æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆé¸æŠã•ã‚ŒãŸPhaseã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‚‚ã®ï¼‰
    df_valid = df_all[df_all[return_col].notna()].copy()

    if len(df_valid) == 0:
        raise HTTPException(status_code=404, detail=f"No valid backtest results found for {phase}")

    # === å…¨ä½“çµ±è¨ˆ ===
    all_returns = df_valid[return_col].tolist()
    all_profits = df_valid[profit_col].tolist() if profit_col in df_valid.columns else ((df_valid['sell_price'] - df_valid['buy_price']) * 100).tolist()

    overall_stats = {
        "total_count": len(df_all),
        "valid_count": len(df_valid),
        "win_count": int((df_valid[win_col] == True).sum()),
        "lose_count": int((df_valid[win_col] == False).sum()),
        "win_rate": float((df_valid[win_col] == True).sum() / len(df_valid) * 100),
        "avg_return": float(sum(all_returns) / len(all_returns) * 100),
        "median_return": float(df_valid[return_col].median() * 100),
        "std_return": float(df_valid[return_col].std() * 100),
        "best_return": float(max(all_returns) * 100),
        "worst_return": float(min(all_returns) * 100),
        "avg_profit_per_100_shares": float(sum(all_profits) / len(all_profits)),
        "total_profit_per_100_shares": float(sum(all_profits)),
        "best_profit_per_100_shares": float(max(all_profits)),
        "worst_profit_per_100_shares": float(min(all_profits)),
        "total_days": int(df_all['backtest_date'].nunique()),
        "phase": phase,
        "phase_description": config["description"],
    }

    # === Top5çµ±è¨ˆ ===
    df_top5 = df_all[df_all['grok_rank'] <= 5]
    df_top5_valid = df_top5[df_top5[return_col].notna()].copy()

    if len(df_top5_valid) > 0:
        top5_returns = df_top5_valid[return_col].tolist()
        top5_profits = df_top5_valid[profit_col].tolist() if profit_col in df_top5_valid.columns else ((df_top5_valid['sell_price'] - df_top5_valid['buy_price']) * 100).tolist()

        top5_stats = {
            "total_count": len(df_top5),
            "valid_count": len(df_top5_valid),
            "win_count": int((df_top5_valid[win_col] == True).sum()),
            "lose_count": int((df_top5_valid[win_col] == False).sum()),
            "win_rate": float((df_top5_valid[win_col] == True).sum() / len(df_top5_valid) * 100),
            "avg_return": float(sum(top5_returns) / len(top5_returns) * 100),
            "median_return": float(df_top5_valid[return_col].median() * 100),
            "std_return": float(df_top5_valid[return_col].std() * 100),
            "best_return": float(max(top5_returns) * 100),
            "worst_return": float(min(top5_returns) * 100),
            "avg_profit_per_100_shares": float(sum(top5_profits) / len(top5_profits)),
            "total_profit_per_100_shares": float(sum(top5_profits)),
            "best_profit_per_100_shares": float(max(top5_profits)),
            "worst_profit_per_100_shares": float(min(top5_profits)),
            "outperformance": float((sum(top5_returns) / len(top5_returns) - sum(all_returns) / len(all_returns)) * 100),
            "outperformance_profit_per_100_shares": float(sum(top5_profits) / len(top5_profits) - sum(all_profits) / len(all_profits)),
        }
    else:
        top5_stats = {
            "total_count": 0,
            "valid_count": 0,
            "win_count": 0,
            "lose_count": 0,
            "win_rate": 0,
            "avg_return": 0,
            "median_return": 0,
            "std_return": 0,
            "best_return": 0,
            "worst_return": 0,
            "avg_profit_per_100_shares": 0,
            "total_profit_per_100_shares": 0,
            "best_profit_per_100_shares": 0,
            "worst_profit_per_100_shares": 0,
            "outperformance": 0,
            "outperformance_profit_per_100_shares": 0,
        }

    # === æ—¥æ¬¡çµ±è¨ˆ ===
    daily_groups = df_all.groupby(df_all['backtest_date'].dt.date)
    daily_stats_list = []

    for backtest_date, df_day in daily_groups:
        df_day_valid = df_day[df_day[return_col].notna()]

        if len(df_day_valid) > 0:
            win_count = (df_day_valid[win_col] == True).sum()
            day_profits = df_day_valid[profit_col].tolist() if profit_col in df_day_valid.columns else ((df_day_valid['sell_price'] - df_day_valid['buy_price']) * 100).tolist()

            # Top5ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—
            df_day_top5 = df_day_valid.nsmallest(5, 'grok_rank') if len(df_day_valid) >= 5 else df_day_valid
            top5_profits = df_day_top5[profit_col].tolist() if profit_col in df_day_top5.columns else ((df_day_top5['sell_price'] - df_day_top5['buy_price']) * 100).tolist()
            top5_win_count = (df_day_top5[win_col] == True).sum()

            daily_stats_list.append({
                "date": backtest_date.isoformat(),
                "win_rate": float(win_count / len(df_day_valid) * 100),
                "avg_return": float(df_day_valid[return_col].mean() * 100),
                "count": len(df_day_valid),
                "total_profit_per_100": float(sum(day_profits)),
                "top5_total_profit_per_100": float(sum(top5_profits)),
                "top5_avg_return": float(df_day_top5[return_col].mean() * 100),
                "top5_win_rate": float(top5_win_count / len(df_day_top5) * 100),
            })

    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
    daily_stats_list.sort(key=lambda x: x["date"])

    # ç´¯ç©æç›Šã‚’è¨ˆç®—
    cumulative_profit = 0.0
    cumulative_top5_profit = 0.0
    for stat in daily_stats_list:
        cumulative_profit += stat["total_profit_per_100"]
        cumulative_top5_profit += stat["top5_total_profit_per_100"]
        stat["cumulative_profit_per_100"] = float(cumulative_profit)
        stat["cumulative_top5_profit_per_100"] = float(cumulative_top5_profit)

    # === ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ ===
    if len(daily_stats_list) > 0:
        recent_days = daily_stats_list[-5:]
        recent_avg = sum(d["win_rate"] for d in recent_days) / len(recent_days)
        overall_avg = sum(d["win_rate"] for d in daily_stats_list) / len(daily_stats_list)
        change = ((recent_avg - overall_avg) / abs(overall_avg) * 100) if overall_avg != 0 else 0

        if change > 10:
            trend = "improving"
        elif change < -10:
            trend = "declining"
        else:
            trend = "stable"

        trend_analysis = {
            "trend": trend,
            "recent_avg": recent_avg,
            "overall_avg": overall_avg,
            "change": change,
        }
    else:
        trend_analysis = {
            "trend": "stable",
            "recent_avg": 0,
            "overall_avg": 0,
            "change": 0,
        }

    # === ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ ===
    alerts = []

    if overall_stats["win_rate"] < 40:
        alerts.append({
            "type": "danger",
            "title": "âš ï¸ å‹ç‡ãŒä½ä¸‹ã—ã¦ã„ã¾ã™",
            "message": f"ç¾åœ¨ã®å‹ç‡: {overall_stats['win_rate']:.1f}%ã€‚æˆ¦ç•¥ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
            "action": "æˆ¦ç•¥ã‚’è¦‹ç›´ã™",
        })
    elif overall_stats["win_rate"] >= 60:
        alerts.append({
            "type": "success",
            "title": "âœ… é«˜ã„å‹ç‡ã‚’ç¶­æŒ",
            "message": f"ç¾åœ¨ã®å‹ç‡: {overall_stats['win_rate']:.1f}%ã€‚æˆ¦ç•¥ã¯é †èª¿ã§ã™ã€‚",
        })

    if trend_analysis["trend"] == "declining":
        alerts.append({
            "type": "warning",
            "title": "ğŸ“‰ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹å‚¾å‘",
            "message": f"ç›´è¿‘5æ—¥ã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³: {trend_analysis['recent_avg']:.2f}%ï¼ˆå…¨æœŸé–“: {trend_analysis['overall_avg']:.2f}%ï¼‰",
            "action": "æ§˜å­è¦‹ã‚’æ¨å¥¨",
        })
    elif trend_analysis["trend"] == "improving":
        alerts.append({
            "type": "success",
            "title": "ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒæ”¹å–„ä¸­",
            "message": f"ç›´è¿‘5æ—¥ã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³: {trend_analysis['recent_avg']:.2f}%ï¼ˆå…¨æœŸé–“: {trend_analysis['overall_avg']:.2f}%ï¼‰",
        })

    if top5_stats["outperformance"] > 0.5:
        alerts.append({
            "type": "success",
            "title": "â­ Top5éŠ˜æŸ„ã¸ã®çµã‚Šè¾¼ã¿ã‚’æ¨å¥¨",
            "message": f"Top5ã¯å…¨ä½“ã‚ˆã‚Šå¹³å‡{top5_stats['outperformance']:.2f}%é«˜ã„ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²ã—ã¦ã„ã¾ã™ã€‚",
            "action": "Top5ã®ã¿ã«ãƒˆãƒ¬ãƒ¼ãƒ‰",
        })

    if overall_stats["valid_count"] < 10:
        alerts.append({
            "type": "warning",
            "title": "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
            "message": f"æœ‰åŠ¹ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ: {overall_stats['valid_count']}ä»¶ã€‚çµ±è¨ˆçš„ãªä¿¡é ¼æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚",
        })

    # === ç›´è¿‘ãƒ¬ã‚³ãƒ¼ãƒ‰ ===
    recent_records = df_all.sort_values('backtest_date', ascending=False).head(10).to_dict(orient='records')

    # NaN, NaT, Timestamp ã‚’ JSON ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
    for record in recent_records:
        for key, value in record.items():
            if pd.isna(value):
                record[key] = None
            elif isinstance(value, pd.Timestamp):
                record[key] = value.isoformat()

    return {
        "overall_stats": overall_stats,
        "top5_stats": top5_stats,
        "daily_stats": daily_stats_list,
        "recent_records": recent_records,
        "trend_analysis": trend_analysis,
        "alerts": alerts,
        "available_versions": available_versions,
        "current_version": current_version,
    }


@router.get("/api/dev/backtest/daily/{date}")
async def get_daily_backtest(date: str):
    """ç‰¹å®šæ—¥ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè©³ç´°"""
    df_all = load_archive_data()

    if df_all.empty:
        raise HTTPException(status_code=404, detail="No backtest data found")

    # æŒ‡å®šæ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    df = df_all[df_all['backtest_date'].dt.date == pd.to_datetime(date).date()]

    if df.empty:
        raise HTTPException(status_code=404, detail=f"No backtest data for {date}")

    stats = calculate_daily_stats(df)

    # çµ±è¨ˆã‚’ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤ºã«å¤‰æ›
    if stats["avg_return"] is not None:
        stats["avg_return"] *= 100
    if stats["max_return"] is not None:
        stats["max_return"] *= 100
    if stats["min_return"] is not None:
        stats["min_return"] *= 100
    if stats["top5_avg_return"] is not None:
        stats["top5_avg_return"] *= 100

    # ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã«å¤‰æ›
    records = []
    for _, row in df.iterrows():
        buy_price = float(row["buy_price"]) if "buy_price" in df.columns and pd.notna(row.get("buy_price")) else None
        sell_price = float(row["sell_price"]) if "sell_price" in df.columns and pd.notna(row.get("sell_price")) else None
        phase1_return = float(row["phase1_return"] * 100) if "phase1_return" in df.columns and pd.notna(row.get("phase1_return")) else None

        # 100æ ªã‚ãŸã‚Šã®åˆ©ç›Šé¡ã‚’è¨ˆç®—
        profit_per_100 = None
        if buy_price is not None and sell_price is not None:
            profit_per_100 = (sell_price - buy_price) * 100

        record = {
            "ticker": row.get("ticker"),
            "stock_name": row.get("stock_name"),
            "selection_score": float(row["selection_score"]) if pd.notna(row.get("selection_score")) else None,
            "grok_rank": int(row["grok_rank"]) if pd.notna(row.get("grok_rank")) else None,
            "reason": row.get("reason"),
            "selected_time": row.get("selected_time"),
            "buy_price": buy_price,
            "sell_price": sell_price,
            "phase1_return": phase1_return,
            "phase1_win": bool(row["phase1_win"]) if "phase1_win" in df.columns and pd.notna(row.get("phase1_win")) else None,
            "profit_per_100": profit_per_100,
            "morning_high": float(row["morning_high"]) if "morning_high" in df.columns and pd.notna(row.get("morning_high")) else None,
            "morning_low": float(row["morning_low"]) if "morning_low" in df.columns and pd.notna(row.get("morning_low")) else None,
            "high": float(row["high"]) if "high" in df.columns and pd.notna(row.get("high")) else None,
            "low": float(row["low"]) if "low" in df.columns and pd.notna(row.get("low")) else None,
            "morning_volume": int(row["morning_volume"]) if "morning_volume" in df.columns and pd.notna(row.get("morning_volume")) else None,
            "max_gain_pct": float(row["max_gain_pct"] * 100) if "max_gain_pct" in df.columns and pd.notna(row.get("max_gain_pct")) else None,
            "max_drawdown_pct": float(row["max_drawdown_pct"] * 100) if "max_drawdown_pct" in df.columns and pd.notna(row.get("max_drawdown_pct")) else None,
        }
        records.append(record)

    return {
        "date": date,
        "stats": stats,
        "results": records,
    }


@router.get("/api/dev/backtest/latest")
async def get_latest_backtest():
    """æœ€æ–°ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ"""
    df_all = load_archive_data()

    if df_all.empty:
        raise HTTPException(status_code=404, detail="No backtest data found")

    # æœ€æ–°æ—¥ä»˜ã‚’å–å¾—
    latest_date = df_all['backtest_date'].max().date()
    return await get_daily_backtest(latest_date.isoformat())


@router.get("/api/dev/backtest/dates")
async def get_available_dates():
    """åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ä¸€è¦§"""
    df_all = load_archive_data()

    if df_all.empty:
        return {"dates": []}

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ—¥ä»˜ã‚’å–å¾—ã—ã¦ã‚½ãƒ¼ãƒˆ
    dates = sorted(df_all['backtest_date'].dt.date.unique(), reverse=True)
    dates_str = [d.isoformat() for d in dates]

    return {"dates": dates_str}
